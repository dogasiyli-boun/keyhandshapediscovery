{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas:  1.1.0\n",
      "torch:  1.6.0\n",
      "numpy:  1.19.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "matplotlib.style.use('ggplot')\n",
    "import sys, importlib as impL\n",
    "import pandas as pd\n",
    "print(\"pandas: \", pd.__version__)\n",
    "print(\"torch: \", torch.__version__)\n",
    "print(\"numpy: \", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsubuntu_khs_dir = /home/wsubuntu/GitHub/keyhandshapediscovery\n",
      "wsubuntu_data_path = /mnt/SSD_Data/DataPath\n",
      "wsubuntu_experiment_path = /mnt/SSD_Data/vaesae_experiments\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "def get_var_by_comp_name(variableName):\n",
    "    curCompName = socket.gethostname()\n",
    "    retVal = None\n",
    "    if variableName == 'khs_dir':\n",
    "        if curCompName == 'doga-MSISSD':\n",
    "            base_dir = '/mnt/USB_HDD_1TB/GitHub/keyhandshapediscovery'  # for bogazici kasa\n",
    "        elif curCompName == 'WsUbuntu05' or curCompName == 'wsubuntu':\n",
    "            base_dir = '/home/wsubuntu/GitHub/keyhandshapediscovery'  # for WS Doga DHO\n",
    "        elif curCompName == 'doga-msi-ubu':\n",
    "            base_dir = None  # for laptop\n",
    "        retVal = base_dir\n",
    "    if variableName == 'data_path':\n",
    "        if curCompName == 'doga-MSISSD':\n",
    "            base_dir = '/mnt/USB_HDD_1TB/Datasets'  # for bogazici kasa\n",
    "        elif curCompName == 'WsUbuntu05' or curCompName == 'wsubuntu':\n",
    "            base_dir = '/mnt/SSD_Data/DataPath'  # for WS Doga DHO\n",
    "        elif curCompName == 'doga-msi-ubu':\n",
    "            base_dir = None  # for laptop\n",
    "        retVal = base_dir\n",
    "    if variableName == 'experiment_path':\n",
    "        if curCompName == 'doga-MSISSD':\n",
    "            base_dir = '/mnt/USB_HDD_1TB/GitHub/keyhandshapediscovery/experiments/SPARSE_TORCH'  # for bogazici kasa\n",
    "        elif curCompName == 'WsUbuntu05' or curCompName == 'wsubuntu':\n",
    "            base_dir = '/mnt/SSD_Data/vaesae_experiments'  # for WS Doga DHO\n",
    "        elif curCompName == 'doga-msi-ubu':\n",
    "            base_dir = None  # for laptop\n",
    "        retVal = base_dir\n",
    "        \n",
    "    print(curCompName + '_' + variableName, '=',  retVal)\n",
    "    if retVal is None:\n",
    "        os.error(5)\n",
    "    return retVal\n",
    "\n",
    "khs_dir = get_var_by_comp_name('khs_dir')\n",
    "data_path = get_var_by_comp_name('data_path')\n",
    "experiment_path = get_var_by_comp_name('experiment_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, khs_dir)\n",
    "import helperFuncs as funcH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_ID = 20\n",
    "LOSS_TYPE='cre'\n",
    "LOSS_REDUCTION='mean' #'batchmean','sum'\n",
    "SIGMOID_ACT=True\n",
    "APPLY_LOG_SOFTMAX=True\n",
    "MSE_PLUS_MINUS='+'\n",
    "APPLY_SPARSITY_TO='bottleneck' #all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bottleneck_acc(bottleneck_vec, lab_vec):\n",
    "    pred_vec = np.argmax(bottleneck_vec.T, axis=0).T.squeeze()\n",
    "    centroid_info_pdf = funcH.get_cluster_centroids(bottleneck_vec, pred_vec, kluster_centers=None, verbose=0)\n",
    "    _confMat_preds, kluster2Classes, kr_pdf, weightedPurity, cnmxh_perc = funcH.countPredictionsForConfusionMat(lab_vec, pred_vec, centroid_info_pdf=centroid_info_pdf, labelNames=None)\n",
    "    sampleCount = np.sum(np.sum(_confMat_preds))\n",
    "    acc = 100 * np.sum(np.diag(_confMat_preds)) / sampleCount\n",
    "    bmx, bmn = np.max(bottleneck_vec), np.min(bottleneck_vec)\n",
    "    return acc, bmx, bmn\n",
    "\n",
    "funcH.setPandasDisplayOpts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the Argument Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add sparsity regularization: yes\n"
     ]
    }
   ],
   "source": [
    "#ap = argparse.ArgumentParser()\n",
    "#ap.add_argument('-e', '--epochs', type=int, default=10, help='number of epochs to train our network for')\n",
    "#ap.add_argument('-l', '--reg_param', type=float, default=0.001, help='regularization parameter `lambda`')\n",
    "#ap.add_argument('-sc', '--add_sparse', type=str, default='yes', help='whether to add sparsity contraint or not')\n",
    "#args = vars(ap.parse_args())\n",
    "epochs = 100  # args['epochs']\n",
    "reg_param = 0.1  # args['reg_param']\n",
    "add_sparsity = 'yes'  # args['add_sparse']\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "print(f\"Add sparsity regularization: {add_sparsity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here I will change the data loader per my need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# get the computation device\n",
    "def get_device():\n",
    "    return 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "FOLDERS = {\n",
    "    \"data\": data_path,\n",
    "    \"experiment\": os.path.join(experiment_path, 'sparse_torch_ae_' + str(EXPERIMENT_ID).zfill(3)),\n",
    "}\n",
    "FOLDERS[\"model_save\"] = os.path.join(FOLDERS[\"experiment\"], \"model\")\n",
    "FOLDERS[\"decoder_image_path_tr\"] = os.path.join(FOLDERS[\"experiment\"], \"output_images_tr\")\n",
    "FOLDERS[\"decoder_image_path_va\"] = os.path.join(FOLDERS[\"experiment\"], \"output_images_va\")\n",
    "funcH.createDirIfNotExist(FOLDERS[\"model_save\"])\n",
    "funcH.createDirIfNotExist(FOLDERS[\"decoder_image_path_tr\"])\n",
    "funcH.createDirIfNotExist(FOLDERS[\"decoder_image_path_va\"])\n",
    "\n",
    "trainset = datasets.FashionMNIST(\n",
    "    root=FOLDERS[\"data\"],\n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "testset = datasets.FashionMNIST(\n",
    "    root=FOLDERS[\"data\"],\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    " \n",
    "# trainloader\n",
    "trainloader = DataLoader(\n",
    "    trainset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "#testloader\n",
    "testloader = DataLoader(\n",
    "    testset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseAutoencoder - loss_type(cre), device(cpu)\n"
     ]
    }
   ],
   "source": [
    "# define the autoencoder model\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, loss_type):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    " \n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=784, out_features=256)\n",
    "        self.enc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.enc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.enc4 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.enc5 = nn.Linear(in_features=32, out_features=32)\n",
    " \n",
    "        # decoder \n",
    "        self.dec1 = nn.Linear(in_features=32, out_features=32)\n",
    "        self.dec2 = nn.Linear(in_features=32, out_features=64)\n",
    "        self.dec3 = nn.Linear(in_features=64, out_features=128)\n",
    "        self.dec4 = nn.Linear(in_features=128, out_features=256)\n",
    "        self.dec5 = nn.Linear(in_features=256, out_features=784)\n",
    "        \n",
    "        self.loss_type=loss_type\n",
    "        self.device = get_device()\n",
    "        print(\"SparseAutoencoder - loss_type(\" + loss_type +\"), device(\" + self.device + \")\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        # encoding\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = F.relu(self.enc4(x))\n",
    "        if self.loss_type=='cre':\n",
    "            bottleneck = self.enc5(x) \n",
    "        else:\n",
    "            bottleneck = F.relu(self.enc5(x))  \n",
    "        \n",
    "        return bottleneck\n",
    "        \n",
    "    def decode(self, bottleneck):\n",
    "        # decoding\n",
    "        if self.loss_type=='cre':\n",
    "            x = F.relu(self.dec1(F.relu(bottleneck)))\n",
    "        else:\n",
    "            x = F.relu(self.dec1(bottleneck))\n",
    "        \n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = F.relu(self.dec3(x))\n",
    "        x = F.relu(self.dec4(x))\n",
    "        x = F.relu(self.dec5(x)) \n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bottleneck = self.encode(x)\n",
    "        x = self.decode(bottleneck)\n",
    "        return x, bottleneck\n",
    "\n",
    "model = SparseAutoencoder(loss_type=LOSS_TYPE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss function\n",
    "criterion = nn.MSELoss()\n",
    "# the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "Linear(in_features=128, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=128, bias=True)\n",
      "Linear(in_features=128, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=784, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the layers as a list\n",
    "model_children = list(model.children())\n",
    "[print(i) for i in model_children]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_l1(bottleneck):\n",
    "    return torch.mean(torch.abs(bottleneck))\n",
    "\n",
    "def loss_l2(bottleneck):\n",
    "    return torch.mean(torch.pow(bottleneck, torch.tensor(2.0).to(device))).sqrt()\n",
    "\n",
    "def kl_divergence(bottleneck, reduction):\n",
    "    bt = torch.sigmoid(bottleneck)  # sigmoid because we need the probability distributions\n",
    "    rho_val = 1/bt.size(1)\n",
    "    rho_mat = torch.tensor([rho_val] * np.ones(bt.size()), dtype=torch.float32).to(device)\n",
    "    #https://discuss.pytorch.org/t/kl-divergence-produces-negative-values/16791/13\n",
    "    #KLDLoss(p, q), sum(q) needs to equal one\n",
    "    #p = log_softmax(tensor)\n",
    "    loss_ret_1 = F.kl_div(F.log_softmax(bt, dim=1).to(device), rho_mat, reduction=reduction)\n",
    "    # torch.sum(rho * torch.log(rho / bottleneck) + (1 - rho) * torch.log((1 - rho) / (1 - bottleneck)))\n",
    "    return loss_ret_1\n",
    "\n",
    "\n",
    "def loss_crossentropy(bottleneck, sigmoidAct, reduction, apply_log_softmax, print_info):\n",
    "    loss_fun = torch.nn.CrossEntropyLoss(reduction=reduction)\n",
    "    if sigmoidAct and apply_log_softmax:  \n",
    "        if print_info:\n",
    "            print(\"1-\",sigmoidAct,apply_log_softmax)\n",
    "        bt = torch.sigmoid(bottleneck)  # sigmoid because we need the probability distributions\n",
    "        _, preds = torch.max(bt, 1)\n",
    "        loss_ret_1 = loss_fun(F.log_softmax(bt, dim=1).to(device), preds)    \n",
    "    elif sigmoidAct and not apply_log_softmax:     \n",
    "        if print_info:\n",
    "            print(\"2-\",sigmoidAct,apply_log_softmax)\n",
    "        bt = torch.sigmoid(bottleneck)  # sigmoid because we need the probability distributions\n",
    "        _, preds = torch.max(bt, 1)\n",
    "        loss_ret_1 = loss_fun(bt.to(device), preds)    \n",
    "    elif not sigmoidAct and apply_log_softmax:\n",
    "        if print_info:\n",
    "            print(\"3-\",sigmoidAct,apply_log_softmax)\n",
    "        _, preds = torch.max(bottleneck, 1)\n",
    "        loss_ret_1 = loss_fun(F.log_softmax(bottleneck, dim=1).to(device), preds)    \n",
    "    else:#nothing\n",
    "        if print_info:\n",
    "            print(\"4-\",sigmoidAct,apply_log_softmax)\n",
    "        _, preds = torch.max(bottleneck, 1)\n",
    "        loss_ret_1 = loss_fun(bottleneck.to(device), preds)\n",
    "    return loss_ret_1\n",
    "\n",
    "def loss_crossentropy_old(bottleneck, sigmoidAct, reduction, apply_log_softmax, print_info):\n",
    "    loss_fun = torch.nn.CrossEntropyLoss(reduction=reduction)\n",
    "    if sigmoidAct:\n",
    "        bt = torch.sigmoid(bottleneck)  # sigmoid because we need the probability distributions\n",
    "    else:\n",
    "        bt = bottleneck    \n",
    "    _, preds = torch.max(bt, 1)\n",
    "    loss_ret_1 = loss_fun(F.log_softmax(bt, dim=1).to(device), preds)    \n",
    "    return loss_ret_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sparse loss function\n",
    "def sparse_loss(autoencoder, images, print_info, loss_type):\n",
    "    loss = 0\n",
    "    values = images\n",
    "    for i in range(len(model_children)):\n",
    "        values = F.relu((model_children[i](values)))\n",
    "        #if print_info:\n",
    "            #print(i, ' shape=', values.shape)\n",
    "        if loss_type=='l1':\n",
    "            loss += loss_l1(values)\n",
    "        if loss_type=='l2':\n",
    "            loss += loss_l2(values)\n",
    "        if loss_type=='kl':\n",
    "            loss += kl_divergence(values, reduction=LOSS_REDUCTION)\n",
    "        if loss_type=='cre':\n",
    "            loss += loss_crossentropy(values, sigmoidAct=SIGMOID_ACT, reduction=LOSS_REDUCTION, apply_log_softmax=APPLY_LOG_SOFTMAX, print_info=print_info)\n",
    "        if print_info:\n",
    "            print(loss_type,loss)\n",
    "    return loss\n",
    "\n",
    "def sparse_loss_bottleneck(bottleneck, print_info, loss_type):\n",
    "    loss = 0\n",
    "    #if print_info:\n",
    "        #print(i, ' shape=', values.shape)\n",
    "    if loss_type=='l1':\n",
    "        loss += loss_l1(bottleneck)\n",
    "    if loss_type=='l2':\n",
    "        loss += loss_l2(bottleneck)\n",
    "    if loss_type=='kl':\n",
    "        loss += kl_divergence(bottleneck, reduction=LOSS_REDUCTION)\n",
    "    if loss_type=='cre':\n",
    "        loss += loss_crossentropy(bottleneck, sigmoidAct=SIGMOID_ACT, reduction=LOSS_REDUCTION, apply_log_softmax=APPLY_LOG_SOFTMAX, print_info=print_info)\n",
    "    if print_info:\n",
    "        print(loss_type,loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_decoded_image(img, name):\n",
    "    img = img.view(img.size(0), 1, 28, 28)\n",
    "    save_image(img, name)\n",
    "\n",
    "# define the training function\n",
    "def fit(model, dataloader, epoch, print_losses_fit):\n",
    "    print('TrEpoch({:03d}) - '.format(epoch), end='')\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    lab_vec = []\n",
    "    bottleneck_vec = []\n",
    "    sparsity_loss_sum = 0\n",
    "    mse_sum = 0\n",
    "       \n",
    "    for data in dataloader:\n",
    "        img, lb = data\n",
    "        lab_vec.append(lb)\n",
    "        \n",
    "        img = img.to(device)\n",
    "        img = img.view(img.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, bottleneck = model(img)\n",
    "        bottleneck_vec.append(bottleneck)\n",
    "        mse_loss = criterion(outputs, img)\n",
    "        mse_sum += mse_loss.item()\n",
    "        #if print_losses_fit:\n",
    "            #print(\"mse_loss:\", mse_loss.to('cpu'))\n",
    "            #print(\"bottleneck:\", bottleneck.to('cpu'))\n",
    "        if add_sparsity == 'yes':\n",
    "            if APPLY_SPARSITY_TO=='all':\n",
    "                sp_loss = sparse_loss(model, img, print_losses_fit, model.loss_type)\n",
    "            elif APPLY_SPARSITY_TO=='bottleneck': #all\n",
    "                sp_loss = sparse_loss_bottleneck(bottleneck, print_losses_fit, model.loss_type)\n",
    "            else:\n",
    "                os.exit(4)\n",
    "                \n",
    "            sparsity_loss_sum += sp_loss.item()\n",
    "            \n",
    "            # add the sparsity penalty\n",
    "            if print_losses_fit:\n",
    "                print(\"sp_loss:\", sparsity_loss_sum)\n",
    "                \n",
    "            if MSE_PLUS_MINUS=='-':\n",
    "                loss = mse_loss - reg_param * sp_loss\n",
    "            elif MSE_PLUS_MINUS=='+':\n",
    "                loss = mse_loss + reg_param * sp_loss\n",
    "        else:\n",
    "            loss = mse_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        print_losses_fit = False\n",
    "    \n",
    "    lab_vec = np.asarray(torch.cat(lab_vec).to(torch.device('cpu')))\n",
    "    bottleneck_vec = np.asarray(torch.cat(bottleneck_vec).to(torch.device('cpu')).detach().numpy())\n",
    "    acc, bmx, bmn = calc_bottleneck_acc(bottleneck_vec, lab_vec)\n",
    "    #print(\"tr bottleneck accuracy=\", acc, \", max=\", bmx, \", min=\", bmn, \", sparsity_loss_sum=\", sparsity_loss_sum)\n",
    "  \n",
    "    result_df = pd.DataFrame(np.array([[acc, bmx, bmn, mse_sum, sparsity_loss_sum, running_loss]]), columns=['acc','bmx','bmn','mse','spr','run'])\n",
    "    #print(df.iloc[0]['mse']) #'acc','bmx','bmn','mse','spr','run'\n",
    "    print(\"\\n\",result_df)\n",
    "    if epoch % 2 == 0:\n",
    "        difn = os.path.join(FOLDERS[\"decoder_image_path_tr\"], \"train\"+str(epoch).zfill(3)+\".png\")\n",
    "        save_decoded_image(outputs.cpu().data, difn)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the validation function\n",
    "def validate(model, dataloader, epoch, print_losses_fit):\n",
    "    print('ValEpoch({:03d}) - '.format(epoch), end='')\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    lab_vec = []\n",
    "    bottleneck_vec = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            img, lb = data\n",
    "            lab_vec.append(lb)\n",
    "            img = img.to(device)\n",
    "            img = img.view(img.size(0), -1)\n",
    "            outputs, bottleneck = model(img)\n",
    "            bottleneck_vec.append(bottleneck)\n",
    "            loss = criterion(outputs, img)\n",
    "            running_loss += loss.item()\n",
    "    # save the reconstructed images every 5 epochs\n",
    "    lab_vec = np.asarray(torch.cat(lab_vec).to(torch.device('cpu')))\n",
    "    bottleneck_vec = np.asarray(torch.cat(bottleneck_vec).to(torch.device('cpu')).detach().numpy())\n",
    "    acc, bmx, bmn = calc_bottleneck_acc(bottleneck_vec, lab_vec)\n",
    "\n",
    "    result_df = pd.DataFrame(np.array([[acc, bmx, bmn, running_loss]]), columns=['acc','bmx','bmn','run'])\n",
    "    print(\"\\n\",result_df)\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        outputs = outputs.view(outputs.size(0), 1, 28, 28).cpu().data\n",
    "        difn = os.path.join(FOLDERS[\"decoder_image_path_va\"], \"reconstruction\"+str(epoch).zfill(3)+\".png\")\n",
    "        save_image(outputs, difn)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stae_ws :: \n",
      "EXPERIMENT_ID:  20\n",
      "LOSS_TYPE :  cre\n",
      "LOSS_REDUCTION :  mean\n",
      "SIGMOID_ACT :  True\n",
      "APPLY_LOG_SOFTMAX :  True\n",
      "APPLY_SPARSITY_TO :  bottleneck\n",
      "total loss = mse_loss + reg_param * sp_loss\n",
      "*****\n",
      " Epoch 0 of 100\n",
      "TrEpoch(000) - 1- True True\n",
      "cre tensor(3.4219, grad_fn=<AddBackward0>)\n",
      "sp_loss: 3.421877145767212\n",
      "\n",
      "     acc    bmx      bmn      mse       spr      run\n",
      "0  10.0  75.08 -152.811  195.515  4817.102  677.225\n",
      "ValEpoch(000) - \n",
      "     acc     bmx      bmn    run\n",
      "0  10.0  74.036 -153.325  29.94\n",
      "*****\n",
      " Epoch 1 of 100\n",
      "TrEpoch(001) - \n",
      "     acc     bmx      bmn      mse       spr      run\n",
      "0  10.0  148.15 -214.054  170.019  4722.498  642.269\n",
      "ValEpoch(001) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  141.829 -209.091  26.462\n",
      "*****\n",
      " Epoch 2 of 100\n",
      "TrEpoch(002) - \n",
      "     acc      bmx     bmn      mse       spr     run\n",
      "0  10.0  170.042 -221.89  150.218  4722.929  622.51\n",
      "ValEpoch(002) - \n",
      "     acc      bmx      bmn    run\n",
      "0  10.0  176.189 -224.949  24.07\n",
      "*****\n",
      " Epoch 3 of 100\n",
      "TrEpoch(003) - \n",
      "     acc      bmx     bmn     mse       spr      run\n",
      "0  10.0  183.501 -227.79  141.17  4722.546  613.424\n",
      "ValEpoch(003) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  171.193 -210.922  22.804\n",
      "*****\n",
      " Epoch 4 of 100\n",
      "TrEpoch(004) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  182.646 -217.089  135.697  4722.375  607.935\n",
      "ValEpoch(004) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  182.063 -217.008  22.178\n",
      "*****\n",
      " Epoch 5 of 100\n",
      "TrEpoch(005) - \n",
      "     acc      bmx      bmn      mse       spr    run\n",
      "0  10.0  182.284 -214.429  132.572  4722.277  604.8\n",
      "ValEpoch(005) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  179.702 -212.243  21.992\n",
      "*****\n",
      " Epoch 6 of 100\n",
      "TrEpoch(006) - 1- True True\n",
      "cre tensor(2.5185, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.5184743404388428\n",
      "\n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  180.267 -212.463  130.629  4722.232  602.852\n",
      "ValEpoch(006) - \n",
      "     acc      bmx      bmn    run\n",
      "0  10.0  173.149 -208.325  21.57\n",
      "*****\n",
      " Epoch 7 of 100\n",
      "TrEpoch(007) - \n",
      "     acc      bmx      bmn     mse       spr     run\n",
      "0  10.0  175.664 -209.712  127.99  4722.304  600.22\n",
      "ValEpoch(007) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  170.208 -211.032  21.086\n",
      "*****\n",
      " Epoch 8 of 100\n",
      "TrEpoch(008) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  174.063 -218.787  125.747  4722.393  597.986\n",
      "ValEpoch(008) - \n",
      "     acc     bmx      bmn     run\n",
      "0  10.0  157.44 -206.598  20.824\n",
      "*****\n",
      " Epoch 9 of 100\n",
      "TrEpoch(009) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  169.894 -224.602  123.862  4722.547  596.117\n",
      "ValEpoch(009) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  161.213 -219.012  20.704\n",
      "*****\n",
      " Epoch 10 of 100\n",
      "TrEpoch(010) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  170.311 -232.302  121.059  4722.455  593.305\n",
      "ValEpoch(010) - \n",
      "     acc     bmx      bmn     run\n",
      "0  10.0  165.95 -227.122  19.926\n",
      "*****\n",
      " Epoch 11 of 100\n",
      "TrEpoch(011) - 1- True True\n",
      "cre tensor(2.5193, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.519254684448242\n",
      "\n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  171.491 -238.828  119.137  4722.372  591.375\n",
      "ValEpoch(011) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  167.654 -234.814  19.656\n",
      "*****\n",
      " Epoch 12 of 100\n",
      "TrEpoch(012) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  180.243 -254.185  116.249  4722.258  588.475\n",
      "ValEpoch(012) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  166.791 -242.148  19.191\n",
      "*****\n",
      " Epoch 13 of 100\n",
      "TrEpoch(013) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  164.875 -249.245  114.549  4722.218  586.771\n",
      "ValEpoch(013) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  164.758 -247.521  19.025\n",
      "*****\n",
      " Epoch 14 of 100\n",
      "TrEpoch(014) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  174.031 -265.131  113.888  4722.174  586.105\n",
      "ValEpoch(014) - \n",
      "     acc      bmx      bmn    run\n",
      "0  10.0  167.488 -256.085  18.91\n",
      "*****\n",
      " Epoch 15 of 100\n",
      "TrEpoch(015) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  173.881 -268.161  112.842  4722.195  585.062\n",
      "ValEpoch(015) - \n",
      "     acc      bmx      bmn    run\n",
      "0  10.0  159.504 -250.039  18.75\n",
      "*****\n",
      " Epoch 16 of 100\n",
      "TrEpoch(016) - 1- True True\n",
      "cre tensor(2.5185, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.518498182296753\n",
      "\n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  177.576 -276.498  112.063  4722.144  584.277\n",
      "ValEpoch(016) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  162.719 -256.687  18.677\n",
      "*****\n",
      " Epoch 17 of 100\n",
      "TrEpoch(017) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  171.578 -270.635  110.68  4722.168  582.897\n",
      "ValEpoch(017) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  158.554 -252.535  18.553\n",
      "*****\n",
      " Epoch 18 of 100\n",
      "TrEpoch(018) - \n",
      "     acc      bmx      bmn      mse       spr     run\n",
      "0  10.0  165.355 -264.759  109.166  4722.141  581.38\n",
      "ValEpoch(018) - \n",
      "     acc      bmx     bmn     run\n",
      "0  10.0  163.064 -259.42  18.195\n",
      "*****\n",
      " Epoch 19 of 100\n",
      "TrEpoch(019) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  175.664 -278.469  108.436  4722.099  580.646\n",
      "ValEpoch(019) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  163.692 -261.388  18.036\n",
      "*****\n",
      " Epoch 20 of 100\n",
      "TrEpoch(020) - \n",
      "     acc      bmx     bmn     mse       spr     run\n",
      "0  10.0  161.275 -260.24  108.16  4722.101  580.37\n",
      "ValEpoch(020) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  148.489 -242.874  18.169\n",
      "*****\n",
      " Epoch 21 of 100\n",
      "TrEpoch(021) - 1- True True\n",
      "cre tensor(2.5184, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.518361806869507\n",
      "\n",
      "     acc      bmx     bmn      mse       spr      run\n",
      "0  10.0  161.944 -261.67  107.598  4722.081  579.806\n",
      "ValEpoch(021) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  155.927 -253.474  17.989\n",
      "*****\n",
      " Epoch 22 of 100\n",
      "TrEpoch(022) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  155.977 -254.194  106.954  4722.064  579.161\n",
      "ValEpoch(022) - \n",
      "     acc      bmx      bmn    run\n",
      "0  10.0  162.024 -263.645  18.02\n",
      "*****\n",
      " Epoch 23 of 100\n",
      "TrEpoch(023) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  161.614 -264.865  106.324  4722.068  578.531\n",
      "ValEpoch(023) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  141.016 -235.723  17.728\n",
      "*****\n",
      " Epoch 24 of 100\n",
      "TrEpoch(024) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  159.329 -263.303  105.443  4722.063  577.649\n",
      "ValEpoch(024) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  135.708 -230.879  17.724\n",
      "*****\n",
      " Epoch 25 of 100\n",
      "TrEpoch(025) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  151.829 -253.552  105.167  4722.047  577.372\n",
      "ValEpoch(025) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  147.782 -247.385  17.548\n",
      "*****\n",
      " Epoch 26 of 100\n",
      "TrEpoch(026) - 1- True True\n",
      "cre tensor(2.5182, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.5182301998138428\n",
      "\n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  154.072 -257.532  104.413  4722.078  576.621\n",
      "ValEpoch(026) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  131.719 -225.947  17.611\n",
      "*****\n",
      " Epoch 27 of 100\n",
      "TrEpoch(027) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  149.839 -252.332  103.794  4722.083  576.002\n",
      "ValEpoch(027) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  129.027 -223.348  17.319\n",
      "*****\n",
      " Epoch 28 of 100\n",
      "TrEpoch(028) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  156.252 -258.564  103.086  4722.078  575.294\n",
      "ValEpoch(028) - \n",
      "     acc      bmx      bmn    run\n",
      "0  10.0  136.394 -232.426  17.33\n",
      "*****\n",
      " Epoch 29 of 100\n",
      "TrEpoch(029) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  149.957 -250.866  103.034  4722.075  575.241\n",
      "ValEpoch(029) - \n",
      "     acc     bmx      bmn     run\n",
      "0  10.0  136.79 -232.895  17.202\n",
      "*****\n",
      " Epoch 30 of 100\n",
      "TrEpoch(030) - \n",
      "     acc      bmx    bmn      mse       spr      run\n",
      "0  10.0  148.412 -246.9  102.839  4722.063  575.045\n",
      "ValEpoch(030) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  142.599 -238.957  17.199\n",
      "*****\n",
      " Epoch 31 of 100\n",
      "TrEpoch(031) - 1- True True\n",
      "cre tensor(2.5184, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.5184357166290283\n",
      "\n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  145.656 -246.997  102.342  4722.086  574.551\n",
      "ValEpoch(031) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  140.166 -236.891  17.182\n",
      "*****\n",
      " Epoch 32 of 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrEpoch(032) - \n",
      "     acc      bmx      bmn      mse      spr      run\n",
      "0  10.0  147.658 -250.945  102.121  4722.05  574.326\n",
      "ValEpoch(032) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  136.283 -232.911  17.184\n",
      "*****\n",
      " Epoch 33 of 100\n",
      "TrEpoch(033) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  143.659 -242.079  102.089  4722.077  574.297\n",
      "ValEpoch(033) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  137.246 -233.199  17.414\n",
      "*****\n",
      " Epoch 34 of 100\n",
      "TrEpoch(034) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  144.118 -241.847  101.894  4722.089  574.103\n",
      "ValEpoch(034) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  136.432 -231.099  17.143\n",
      "*****\n",
      " Epoch 35 of 100\n",
      "TrEpoch(035) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  147.149 -247.664  101.779  4722.053  573.984\n",
      "ValEpoch(035) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  133.397 -227.047  17.126\n",
      "*****\n",
      " Epoch 36 of 100\n",
      "TrEpoch(036) - 1- True True\n",
      "cre tensor(2.5183, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.5182700157165527\n",
      "\n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  145.505 -244.896  101.47  4722.063  573.676\n",
      "ValEpoch(036) - \n",
      "     acc      bmx     bmn     run\n",
      "0  10.0  135.447 -228.49  17.135\n",
      "*****\n",
      " Epoch 37 of 100\n",
      "TrEpoch(037) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  143.115 -238.623  101.295  4722.059  573.501\n",
      "ValEpoch(037) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  137.461 -229.688  16.988\n",
      "*****\n",
      " Epoch 38 of 100\n",
      "TrEpoch(038) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  149.383 -246.748  101.077  4722.073  573.284\n",
      "ValEpoch(038) - \n",
      "     acc      bmx     bmn     run\n",
      "0  10.0  145.468 -240.56  17.105\n",
      "*****\n",
      " Epoch 39 of 100\n",
      "TrEpoch(039) - \n",
      "     acc     bmx      bmn      mse       spr      run\n",
      "0  10.0  149.23 -245.095  100.947  4722.076  573.154\n",
      "ValEpoch(039) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  150.786 -247.713  16.981\n",
      "*****\n",
      " Epoch 40 of 100\n",
      "TrEpoch(040) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  151.169 -247.211  100.752  4722.055  572.957\n",
      "ValEpoch(040) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  147.352 -242.509  17.039\n",
      "*****\n",
      " Epoch 41 of 100\n",
      "TrEpoch(041) - 1- True True\n",
      "cre tensor(2.5181, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.5181026458740234\n",
      "\n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  145.437 -240.333  100.796  4722.075  573.003\n",
      "ValEpoch(041) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  137.483 -226.524  16.929\n",
      "*****\n",
      " Epoch 42 of 100\n",
      "TrEpoch(042) - \n",
      "     acc      bmx      bmn      mse       spr      run\n",
      "0  10.0  151.784 -249.645  100.451  4722.093  572.661\n",
      "ValEpoch(042) - \n",
      "     acc     bmx      bmn     run\n",
      "0  10.0  144.65 -235.989  16.918\n",
      "*****\n",
      " Epoch 43 of 100\n",
      "TrEpoch(043) - \n",
      "     acc     bmx      bmn     mse       spr      run\n",
      "0  10.0  155.81 -254.296  99.804  4722.094  572.014\n",
      "ValEpoch(043) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  138.876 -229.195  16.802\n",
      "*****\n",
      " Epoch 44 of 100\n",
      "TrEpoch(044) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  150.196 -244.443  99.499  4722.067  571.706\n",
      "ValEpoch(044) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  148.071 -241.616  16.743\n",
      "*****\n",
      " Epoch 45 of 100\n",
      "TrEpoch(045) - \n",
      "     acc      bmx      bmn    mse       spr      run\n",
      "0  10.0  153.495 -248.435  99.22  4722.086  571.429\n",
      "ValEpoch(045) - \n",
      "     acc     bmx      bmn     run\n",
      "0  10.0  145.14 -236.894  16.686\n",
      "*****\n",
      " Epoch 46 of 100\n",
      "TrEpoch(046) - 1- True True\n",
      "cre tensor(2.5188, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.518803834915161\n",
      "\n",
      "     acc     bmx      bmn     mse       spr      run\n",
      "0  10.0  152.85 -247.043  99.149  4722.038  571.353\n",
      "ValEpoch(046) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  144.666 -234.177  16.737\n",
      "*****\n",
      " Epoch 47 of 100\n",
      "TrEpoch(047) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  151.046 -243.217  99.127  4722.074  571.334\n",
      "ValEpoch(047) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  143.134 -231.063  16.765\n",
      "*****\n",
      " Epoch 48 of 100\n",
      "TrEpoch(048) - \n",
      "     acc      bmx      bmn     mse       spr     run\n",
      "0  10.0  152.648 -244.763  98.711  4722.082  570.92\n",
      "ValEpoch(048) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  149.698 -239.678  16.762\n",
      "*****\n",
      " Epoch 49 of 100\n",
      "TrEpoch(049) - \n",
      "     acc      bmx      bmn    mse       spr      run\n",
      "0  10.0  158.317 -251.867  98.58  4722.088  570.789\n",
      "ValEpoch(049) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  146.154 -232.732  16.609\n",
      "*****\n",
      " Epoch 50 of 100\n",
      "TrEpoch(050) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  176.207 -276.666  97.964  4722.113  570.175\n",
      "ValEpoch(050) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  154.365 -244.616  16.356\n",
      "*****\n",
      " Epoch 51 of 100\n",
      "TrEpoch(051) - 1- True True\n",
      "cre tensor(2.5182, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.518242359161377\n",
      "\n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  157.796 -252.021  96.904  4722.119  569.116\n",
      "ValEpoch(051) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  139.858 -223.071  16.347\n",
      "*****\n",
      " Epoch 52 of 100\n",
      "TrEpoch(052) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  169.309 -265.854  96.834  4722.119  569.046\n",
      "ValEpoch(052) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  145.827 -230.837  16.272\n",
      "*****\n",
      " Epoch 53 of 100\n",
      "TrEpoch(053) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  174.583 -271.642  96.646  4722.078  568.854\n",
      "ValEpoch(053) - \n",
      "     acc      bmx     bmn     run\n",
      "0  10.0  158.036 -246.83  16.308\n",
      "*****\n",
      " Epoch 54 of 100\n",
      "TrEpoch(054) - \n",
      "     acc      bmx     bmn     mse       spr     run\n",
      "0  10.0  182.356 -281.61  96.709  4722.108  568.92\n",
      "ValEpoch(054) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  159.483 -248.836  16.259\n",
      "*****\n",
      " Epoch 55 of 100\n",
      "TrEpoch(055) - \n",
      "     acc      bmx      bmn    mse       spr     run\n",
      "0  10.0  164.553 -257.468  96.25  4722.091  568.46\n",
      "ValEpoch(055) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  162.257 -251.131  16.249\n",
      "*****\n",
      " Epoch 56 of 100\n",
      "TrEpoch(056) - 1- True True\n",
      "cre tensor(2.5185, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.5185294151306152\n",
      "\n",
      "     acc      bmx      bmn    mse       spr      run\n",
      "0  10.0  173.103 -266.986  96.16  4722.088  568.368\n",
      "ValEpoch(056) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  159.722 -248.703  16.184\n",
      "*****\n",
      " Epoch 57 of 100\n",
      "TrEpoch(057) - \n",
      "     acc     bmx      bmn     mse       spr      run\n",
      "0  10.0  169.35 -260.863  95.697  4722.111  567.908\n",
      "ValEpoch(057) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  162.219 -249.978  16.159\n",
      "*****\n",
      " Epoch 58 of 100\n",
      "TrEpoch(058) - \n",
      "     acc     bmx      bmn     mse       spr      run\n",
      "0  10.0  176.14 -270.321  95.643  4722.082  567.851\n",
      "ValEpoch(058) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  169.453 -259.772  16.123\n",
      "*****\n",
      " Epoch 59 of 100\n",
      "TrEpoch(059) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  171.392 -261.461  95.288  4722.072  567.495\n",
      "ValEpoch(059) - \n",
      "     acc      bmx     bmn     run\n",
      "0  10.0  157.924 -243.24  16.087\n",
      "*****\n",
      " Epoch 60 of 100\n",
      "TrEpoch(060) - \n",
      "     acc      bmx      bmn     mse       spr     run\n",
      "0  10.0  195.124 -296.323  95.082  4722.085  567.29\n",
      "ValEpoch(060) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  167.797 -256.692  16.071\n",
      "*****\n",
      " Epoch 61 of 100\n",
      "TrEpoch(061) - 1- True True\n",
      "cre tensor(2.5183, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.5182530879974365\n",
      "\n",
      "     acc      bmx     bmn     mse       spr      run\n",
      "0  10.0  177.803 -271.87  94.277  4722.077  566.485\n",
      "ValEpoch(061) - \n",
      "     acc     bmx      bmn     run\n",
      "0  10.0  179.91 -273.908  15.763\n",
      "*****\n",
      " Epoch 62 of 100\n",
      "TrEpoch(062) - \n",
      "     acc     bmx      bmn     mse       spr      run\n",
      "0  10.0  177.45 -270.349  93.462  4722.108  565.673\n",
      "ValEpoch(062) - \n",
      "     acc     bmx      bmn     run\n",
      "0  10.0  172.45 -262.795  15.857\n",
      "*****\n",
      " Epoch 63 of 100\n",
      "TrEpoch(063) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  173.254 -263.214  93.816  4722.113  566.027\n",
      "ValEpoch(063) - \n",
      "     acc      bmx      bmn    run\n",
      "0  10.0  172.377 -262.356  15.71\n",
      "*****\n",
      " Epoch 64 of 100\n",
      "TrEpoch(064) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  187.849 -285.549  93.352  4722.096  565.562\n",
      "ValEpoch(064) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  181.685 -276.043  15.736\n",
      "*****\n",
      " Epoch 65 of 100\n",
      "TrEpoch(065) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  175.187 -267.573  93.416  4722.134  565.629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValEpoch(065) - \n",
      "     acc     bmx      bmn     run\n",
      "0  10.0  173.67 -265.071  15.681\n",
      "*****\n",
      " Epoch 66 of 100\n",
      "TrEpoch(066) - 1- True True\n",
      "cre tensor(2.5183, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.5183074474334717\n",
      "\n",
      "     acc     bmx      bmn     mse       spr     run\n",
      "0  10.0  187.32 -284.904  93.106  4722.135  565.32\n",
      "ValEpoch(066) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  171.862 -263.329  15.689\n",
      "*****\n",
      " Epoch 67 of 100\n",
      "TrEpoch(067) - \n",
      "     acc      bmx      bmn    mse       spr      run\n",
      "0  10.0  187.503 -284.635  93.19  4722.134  565.403\n",
      "ValEpoch(067) - \n",
      "     acc     bmx      bmn     run\n",
      "0  10.0  177.88 -271.603  15.592\n",
      "*****\n",
      " Epoch 68 of 100\n",
      "TrEpoch(068) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  182.136 -278.762  93.167  4722.113  565.378\n",
      "ValEpoch(068) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  167.234 -256.704  15.472\n",
      "*****\n",
      " Epoch 69 of 100\n",
      "TrEpoch(069) - \n",
      "     acc      bmx      bmn    mse       spr      run\n",
      "0  10.0  182.232 -279.022  92.12  4722.136  564.334\n",
      "ValEpoch(069) - \n",
      "     acc      bmx      bmn    run\n",
      "0  10.0  173.106 -265.006  15.43\n",
      "*****\n",
      " Epoch 70 of 100\n",
      "TrEpoch(070) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  181.247 -276.438  91.978  4722.154  564.194\n",
      "ValEpoch(070) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  164.758 -253.147  15.522\n",
      "*****\n",
      " Epoch 71 of 100\n",
      "TrEpoch(071) - 1- True True\n",
      "cre tensor(2.5185, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.5185048580169678\n",
      "\n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  173.609 -266.175  91.233  4722.188  563.452\n",
      "ValEpoch(071) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  160.743 -248.681  15.398\n",
      "*****\n",
      " Epoch 72 of 100\n",
      "TrEpoch(072) - \n",
      "     acc      bmx      bmn    mse       spr      run\n",
      "0  10.0  175.517 -272.128  91.02  4722.189  563.239\n",
      "ValEpoch(072) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  164.012 -254.953  15.396\n",
      "*****\n",
      " Epoch 73 of 100\n",
      "TrEpoch(073) - \n",
      "     acc      bmx    bmn     mse       spr      run\n",
      "0  10.0  180.587 -277.2  91.118  4722.205  563.339\n",
      "ValEpoch(073) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  164.902 -254.221  15.394\n",
      "*****\n",
      " Epoch 74 of 100\n",
      "TrEpoch(074) - \n",
      "     acc      bmx      bmn     mse      spr      run\n",
      "0  10.0  180.701 -278.512  90.668  4722.17  562.885\n",
      "ValEpoch(074) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  176.898 -274.161  15.311\n",
      "*****\n",
      " Epoch 75 of 100\n",
      "TrEpoch(075) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  175.021 -268.861  91.186  4722.197  563.406\n",
      "ValEpoch(075) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  159.359 -248.398  15.548\n",
      "*****\n",
      " Epoch 76 of 100\n",
      "TrEpoch(076) - 1- True True\n",
      "cre tensor(2.5184, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.5183916091918945\n",
      "\n",
      "     acc      bmx      bmn    mse       spr      run\n",
      "0  10.0  171.617 -269.917  90.27  4722.235  562.494\n",
      "ValEpoch(076) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  162.678 -256.941  15.524\n",
      "*****\n",
      " Epoch 77 of 100\n",
      "TrEpoch(077) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  179.016 -281.862  89.797  4722.251  562.022\n",
      "ValEpoch(077) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  169.418 -266.777  14.977\n",
      "*****\n",
      " Epoch 78 of 100\n",
      "TrEpoch(078) - \n",
      "     acc      bmx      bmn    mse       spr      run\n",
      "0  10.0  175.776 -276.705  89.91  4722.251  562.136\n",
      "ValEpoch(078) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  149.082 -238.676  15.195\n",
      "*****\n",
      " Epoch 79 of 100\n",
      "TrEpoch(079) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  168.334 -270.831  89.289  4722.301  561.519\n",
      "ValEpoch(079) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  158.096 -254.621  15.403\n",
      "*****\n",
      " Epoch 80 of 100\n",
      "TrEpoch(080) - \n",
      "     acc     bmx      bmn     mse       spr      run\n",
      "0  10.0  163.31 -262.555  88.837  4722.346  561.072\n",
      "ValEpoch(080) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  166.273 -268.139  15.077\n",
      "*****\n",
      " Epoch 81 of 100\n",
      "TrEpoch(081) - 1- True True\n",
      "cre tensor(2.5192, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.5192184448242188\n",
      "\n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  167.507 -269.354  89.087  4722.305  561.317\n",
      "ValEpoch(081) - \n",
      "     acc      bmx      bmn    run\n",
      "0  10.0  158.497 -258.744  15.02\n",
      "*****\n",
      " Epoch 82 of 100\n",
      "TrEpoch(082) - \n",
      "     acc     bmx      bmn     mse       spr      run\n",
      "0  10.0  158.21 -255.858  88.647  4722.362  560.883\n",
      "ValEpoch(082) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  155.844 -256.918  14.909\n",
      "*****\n",
      " Epoch 83 of 100\n",
      "TrEpoch(083) - \n",
      "     acc      bmx    bmn     mse       spr      run\n",
      "0  10.0  159.654 -262.1  88.451  4722.379  560.689\n",
      "ValEpoch(083) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  157.073 -259.576  14.862\n",
      "*****\n",
      " Epoch 84 of 100\n",
      "TrEpoch(084) - \n",
      "     acc      bmx      bmn     mse      spr      run\n",
      "0  10.0  160.693 -265.471  88.539  4722.38  560.777\n",
      "ValEpoch(084) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  162.933 -269.362  14.972\n",
      "*****\n",
      " Epoch 85 of 100\n",
      "TrEpoch(085) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  159.203 -263.963  88.264  4722.349  560.499\n",
      "ValEpoch(085) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  149.642 -249.718  14.791\n",
      "*****\n",
      " Epoch 86 of 100\n",
      "TrEpoch(086) - 1- True True\n",
      "cre tensor(2.5191, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.5191280841827393\n",
      "\n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  148.089 -247.558  87.685  4722.374  559.923\n",
      "ValEpoch(086) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  142.221 -239.506  14.652\n",
      "*****\n",
      " Epoch 87 of 100\n",
      "TrEpoch(087) - \n",
      "     acc     bmx      bmn     mse       spr      run\n",
      "0  10.0  149.18 -249.549  87.826  4722.368  560.063\n",
      "ValEpoch(087) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  142.215 -240.994  14.682\n",
      "*****\n",
      " Epoch 88 of 100\n",
      "TrEpoch(088) - \n",
      "     acc      bmx      bmn     mse       spr     run\n",
      "0  10.0  138.637 -236.609  87.647  4722.427  559.89\n",
      "ValEpoch(088) - \n",
      "     acc      bmx     bmn     run\n",
      "0  10.0  136.778 -233.79  14.714\n",
      "*****\n",
      " Epoch 89 of 100\n",
      "TrEpoch(089) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  143.406 -245.648  87.004  4722.402  559.245\n",
      "ValEpoch(089) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  139.957 -241.395  14.686\n",
      "*****\n",
      " Epoch 90 of 100\n",
      "TrEpoch(090) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  140.035 -242.047  87.176  4722.374  559.414\n",
      "ValEpoch(090) - \n",
      "     acc     bmx      bmn     run\n",
      "0  10.0  138.19 -240.815  14.847\n",
      "*****\n",
      " Epoch 91 of 100\n",
      "TrEpoch(091) - 1- True True\n",
      "cre tensor(2.5190, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.5190279483795166\n",
      "\n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  138.329 -240.186  87.535  4722.339  559.769\n",
      "ValEpoch(091) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  132.918 -232.321  14.707\n",
      "*****\n",
      " Epoch 92 of 100\n",
      "TrEpoch(092) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  134.753 -236.189  86.744  4722.385  558.983\n",
      "ValEpoch(092) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  135.906 -238.334  14.973\n",
      "*****\n",
      " Epoch 93 of 100\n",
      "TrEpoch(093) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  133.141 -238.141  87.129  4722.369  559.366\n",
      "ValEpoch(093) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  124.179 -226.958  14.731\n",
      "*****\n",
      " Epoch 94 of 100\n",
      "TrEpoch(094) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  124.179 -225.165  86.307  4722.509  558.558\n",
      "ValEpoch(094) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  114.893 -209.645  14.539\n",
      "*****\n",
      " Epoch 95 of 100\n",
      "TrEpoch(095) - \n",
      "     acc     bmx      bmn     mse       spr      run\n",
      "0  10.0  119.59 -217.098  85.932  4722.462  558.178\n",
      "ValEpoch(095) - \n",
      "     acc      bmx      bmn    run\n",
      "0  10.0  100.813 -186.028  15.41\n",
      "*****\n",
      " Epoch 96 of 100\n",
      "TrEpoch(096) - 1- True True\n",
      "cre tensor(2.5190, grad_fn=<AddBackward0>)\n",
      "sp_loss: 2.5189614295959473\n",
      "\n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  112.137 -207.527  85.501  4722.444  557.745\n",
      "ValEpoch(096) - \n",
      "     acc      bmx      bmn     run\n",
      "0  10.0  101.241 -188.916  15.143\n",
      "*****\n",
      " Epoch 97 of 100\n",
      "TrEpoch(097) - \n",
      "     acc      bmx     bmn     mse       spr      run\n",
      "0  10.0  113.091 -212.15  85.791  4722.456  558.037\n",
      "ValEpoch(097) - \n",
      "     acc      bmx      bmn    run\n",
      "0  10.0  105.709 -197.596  14.23\n",
      "*****\n",
      " Epoch 98 of 100\n",
      "TrEpoch(098) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  107.649 -200.449  85.681  4722.516  557.932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValEpoch(098) - \n",
      "     acc     bmx      bmn     run\n",
      "0  10.0  103.54 -194.158  14.305\n",
      "*****\n",
      " Epoch 99 of 100\n",
      "TrEpoch(099) - \n",
      "     acc      bmx      bmn     mse       spr      run\n",
      "0  10.0  107.857 -203.072  84.879  4722.516  557.131\n",
      "ValEpoch(099) - \n",
      "     acc     bmx      bmn     run\n",
      "0  10.0  97.484 -184.029  14.391\n",
      "1.66e+02 minutes\n"
     ]
    }
   ],
   "source": [
    "# train and validate the autoencoder neural network\n",
    "start = time.time()\n",
    "print_losses_fit = True\n",
    "\n",
    "train_loss = []\n",
    "trn_spars_loss = []\n",
    "trn_bot_acc = []\n",
    "val_loss = []\n",
    "val_bot_acc = []\n",
    "\n",
    "result_df_tr_all = pd.DataFrame(columns=['acc','bmx','bmn','mse','spr','run'])\n",
    "result_df_va_all = pd.DataFrame(columns=['acc','bmx','bmn','run'])\n",
    "\n",
    "print(\"stae_ws :: \")\n",
    "print(\"EXPERIMENT_ID: \", EXPERIMENT_ID)\n",
    "print(\"LOSS_TYPE : \", LOSS_TYPE)\n",
    "print(\"LOSS_REDUCTION : \", LOSS_REDUCTION)\n",
    "print(\"SIGMOID_ACT : \", SIGMOID_ACT)\n",
    "print(\"APPLY_LOG_SOFTMAX : \", APPLY_LOG_SOFTMAX)\n",
    "print(\"APPLY_SPARSITY_TO : \", APPLY_SPARSITY_TO)\n",
    "print(\"total loss = mse_loss \" + MSE_PLUS_MINUS + \" reg_param * sp_loss\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"*****\\n Epoch {epoch} of {epochs}\")\n",
    "    result_df_tr = fit(model, trainloader, epoch, print_losses_fit)\n",
    "    result_df_va = validate(model, testloader, epoch, print_losses_fit)\n",
    "    print_losses_fit = epoch%5==0 and epoch>0\n",
    "    result_df_tr_all = result_df_tr_all.append(result_df_tr, ignore_index=True)\n",
    "    result_df_va_all = result_df_va_all.append(result_df_va, ignore_index=True)\n",
    "    \n",
    "end = time.time()\n",
    " \n",
    "print(f\"{(end-start)/60:.3} minutes\")\n",
    "# save the trained model\n",
    "\n",
    "mofn = os.path.join(FOLDERS[\"model_save\"], \"sparse_ae_\"+str(epoch).zfill(3)+\".pth\")\n",
    "torch.save(model.state_dict(), mofn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     acc      bmx      bmn      mse       spr      run\n",
      "0   10.0   75.080 -152.811  195.515  4817.102  677.225\n",
      "1   10.0  148.150 -214.054  170.019  4722.498  642.269\n",
      "2   10.0  170.042 -221.890  150.218  4722.929  622.510\n",
      "3   10.0  183.501 -227.790  141.170  4722.546  613.424\n",
      "4   10.0  182.646 -217.089  135.697  4722.375  607.935\n",
      "5   10.0  182.284 -214.429  132.572  4722.277  604.800\n",
      "6   10.0  180.267 -212.463  130.629  4722.232  602.852\n",
      "7   10.0  175.664 -209.712  127.990  4722.304  600.220\n",
      "8   10.0  174.063 -218.787  125.747  4722.393  597.986\n",
      "9   10.0  169.894 -224.602  123.862  4722.547  596.117\n",
      "10  10.0  170.311 -232.302  121.059  4722.455  593.305\n",
      "11  10.0  171.491 -238.828  119.137  4722.372  591.375\n",
      "12  10.0  180.243 -254.185  116.249  4722.258  588.475\n",
      "13  10.0  164.875 -249.245  114.549  4722.218  586.771\n",
      "14  10.0  174.031 -265.131  113.888  4722.174  586.105\n",
      "15  10.0  173.881 -268.161  112.842  4722.195  585.062\n",
      "16  10.0  177.576 -276.498  112.063  4722.144  584.277\n",
      "17  10.0  171.578 -270.635  110.680  4722.168  582.897\n",
      "18  10.0  165.355 -264.759  109.166  4722.141  581.380\n",
      "19  10.0  175.664 -278.469  108.436  4722.099  580.646\n",
      "20  10.0  161.275 -260.240  108.160  4722.101  580.370\n",
      "21  10.0  161.944 -261.670  107.598  4722.081  579.806\n",
      "22  10.0  155.977 -254.194  106.954  4722.064  579.161\n",
      "23  10.0  161.614 -264.865  106.324  4722.068  578.531\n",
      "24  10.0  159.329 -263.303  105.443  4722.063  577.649\n",
      "25  10.0  151.829 -253.552  105.167  4722.047  577.372\n",
      "26  10.0  154.072 -257.532  104.413  4722.078  576.621\n",
      "27  10.0  149.839 -252.332  103.794  4722.083  576.002\n",
      "28  10.0  156.252 -258.564  103.086  4722.078  575.294\n",
      "29  10.0  149.957 -250.866  103.034  4722.075  575.241\n",
      "30  10.0  148.412 -246.900  102.839  4722.063  575.045\n",
      "31  10.0  145.656 -246.997  102.342  4722.086  574.551\n",
      "32  10.0  147.658 -250.945  102.121  4722.050  574.326\n",
      "33  10.0  143.659 -242.079  102.089  4722.077  574.297\n",
      "34  10.0  144.118 -241.847  101.894  4722.089  574.103\n",
      "35  10.0  147.149 -247.664  101.779  4722.053  573.984\n",
      "36  10.0  145.505 -244.896  101.470  4722.063  573.676\n",
      "37  10.0  143.115 -238.623  101.295  4722.059  573.501\n",
      "38  10.0  149.383 -246.748  101.077  4722.073  573.284\n",
      "39  10.0  149.230 -245.095  100.947  4722.076  573.154\n",
      "40  10.0  151.169 -247.211  100.752  4722.055  572.957\n",
      "41  10.0  145.437 -240.333  100.796  4722.075  573.003\n",
      "42  10.0  151.784 -249.645  100.451  4722.093  572.661\n",
      "43  10.0  155.810 -254.296   99.804  4722.094  572.014\n",
      "44  10.0  150.196 -244.443   99.499  4722.067  571.706\n",
      "45  10.0  153.495 -248.435   99.220  4722.086  571.429\n",
      "46  10.0  152.850 -247.043   99.149  4722.038  571.353\n",
      "47  10.0  151.046 -243.217   99.127  4722.074  571.334\n",
      "48  10.0  152.648 -244.763   98.711  4722.082  570.920\n",
      "49  10.0  158.317 -251.867   98.580  4722.088  570.789\n",
      "50  10.0  176.207 -276.666   97.964  4722.113  570.175\n",
      "51  10.0  157.796 -252.021   96.904  4722.119  569.116\n",
      "52  10.0  169.309 -265.854   96.834  4722.119  569.046\n",
      "53  10.0  174.583 -271.642   96.646  4722.078  568.854\n",
      "54  10.0  182.356 -281.610   96.709  4722.108  568.920\n",
      "55  10.0  164.553 -257.468   96.250  4722.091  568.460\n",
      "56  10.0  173.103 -266.986   96.160  4722.088  568.368\n",
      "57  10.0  169.350 -260.863   95.697  4722.111  567.908\n",
      "58  10.0  176.140 -270.321   95.643  4722.082  567.851\n",
      "59  10.0  171.392 -261.461   95.288  4722.072  567.495\n",
      "60  10.0  195.124 -296.323   95.082  4722.085  567.290\n",
      "61  10.0  177.803 -271.870   94.277  4722.077  566.485\n",
      "62  10.0  177.450 -270.349   93.462  4722.108  565.673\n",
      "63  10.0  173.254 -263.214   93.816  4722.113  566.027\n",
      "64  10.0  187.849 -285.549   93.352  4722.096  565.562\n",
      "65  10.0  175.187 -267.573   93.416  4722.134  565.629\n",
      "66  10.0  187.320 -284.904   93.106  4722.135  565.320\n",
      "67  10.0  187.503 -284.635   93.190  4722.134  565.403\n",
      "68  10.0  182.136 -278.762   93.167  4722.113  565.378\n",
      "69  10.0  182.232 -279.022   92.120  4722.136  564.334\n",
      "70  10.0  181.247 -276.438   91.978  4722.154  564.194\n",
      "71  10.0  173.609 -266.175   91.233  4722.188  563.452\n",
      "72  10.0  175.517 -272.128   91.020  4722.189  563.239\n",
      "73  10.0  180.587 -277.200   91.118  4722.205  563.339\n",
      "74  10.0  180.701 -278.512   90.668  4722.170  562.885\n",
      "75  10.0  175.021 -268.861   91.186  4722.197  563.406\n",
      "76  10.0  171.617 -269.917   90.270  4722.235  562.494\n",
      "77  10.0  179.016 -281.862   89.797  4722.251  562.022\n",
      "78  10.0  175.776 -276.705   89.910  4722.251  562.136\n",
      "79  10.0  168.334 -270.831   89.289  4722.301  561.519\n",
      "80  10.0  163.310 -262.555   88.837  4722.346  561.072\n",
      "81  10.0  167.507 -269.354   89.087  4722.305  561.317\n",
      "82  10.0  158.210 -255.858   88.647  4722.362  560.883\n",
      "83  10.0  159.654 -262.100   88.451  4722.379  560.689\n",
      "84  10.0  160.693 -265.471   88.539  4722.380  560.777\n",
      "85  10.0  159.203 -263.963   88.264  4722.349  560.499\n",
      "86  10.0  148.089 -247.558   87.685  4722.374  559.923\n",
      "87  10.0  149.180 -249.549   87.826  4722.368  560.063\n",
      "88  10.0  138.637 -236.609   87.647  4722.427  559.890\n",
      "89  10.0  143.406 -245.648   87.004  4722.402  559.245\n",
      "90  10.0  140.035 -242.047   87.176  4722.374  559.414\n",
      "91  10.0  138.329 -240.186   87.535  4722.339  559.769\n",
      "92  10.0  134.753 -236.189   86.744  4722.385  558.983\n",
      "93  10.0  133.141 -238.141   87.129  4722.369  559.366\n",
      "94  10.0  124.179 -225.165   86.307  4722.509  558.558\n",
      "95  10.0  119.590 -217.098   85.932  4722.462  558.178\n",
      "96  10.0  112.137 -207.527   85.501  4722.444  557.745\n",
      "97  10.0  113.091 -212.150   85.791  4722.456  558.037\n",
      "98  10.0  107.649 -200.449   85.681  4722.516  557.932\n",
      "99  10.0  107.857 -203.072   84.879  4722.516  557.131\n"
     ]
    }
   ],
   "source": [
    "print(result_df_tr_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     acc      bmx      bmn     run\n",
      "0   10.0   74.036 -153.325  29.940\n",
      "1   10.0  141.829 -209.091  26.462\n",
      "2   10.0  176.189 -224.949  24.070\n",
      "3   10.0  171.193 -210.922  22.804\n",
      "4   10.0  182.063 -217.008  22.178\n",
      "5   10.0  179.702 -212.243  21.992\n",
      "6   10.0  173.149 -208.325  21.570\n",
      "7   10.0  170.208 -211.032  21.086\n",
      "8   10.0  157.440 -206.598  20.824\n",
      "9   10.0  161.213 -219.012  20.704\n",
      "10  10.0  165.950 -227.122  19.926\n",
      "11  10.0  167.654 -234.814  19.656\n",
      "12  10.0  166.791 -242.148  19.191\n",
      "13  10.0  164.758 -247.521  19.025\n",
      "14  10.0  167.488 -256.085  18.910\n",
      "15  10.0  159.504 -250.039  18.750\n",
      "16  10.0  162.719 -256.687  18.677\n",
      "17  10.0  158.554 -252.535  18.553\n",
      "18  10.0  163.064 -259.420  18.195\n",
      "19  10.0  163.692 -261.388  18.036\n",
      "20  10.0  148.489 -242.874  18.169\n",
      "21  10.0  155.927 -253.474  17.989\n",
      "22  10.0  162.024 -263.645  18.020\n",
      "23  10.0  141.016 -235.723  17.728\n",
      "24  10.0  135.708 -230.879  17.724\n",
      "25  10.0  147.782 -247.385  17.548\n",
      "26  10.0  131.719 -225.947  17.611\n",
      "27  10.0  129.027 -223.348  17.319\n",
      "28  10.0  136.394 -232.426  17.330\n",
      "29  10.0  136.790 -232.895  17.202\n",
      "30  10.0  142.599 -238.957  17.199\n",
      "31  10.0  140.166 -236.891  17.182\n",
      "32  10.0  136.283 -232.911  17.184\n",
      "33  10.0  137.246 -233.199  17.414\n",
      "34  10.0  136.432 -231.099  17.143\n",
      "35  10.0  133.397 -227.047  17.126\n",
      "36  10.0  135.447 -228.490  17.135\n",
      "37  10.0  137.461 -229.688  16.988\n",
      "38  10.0  145.468 -240.560  17.105\n",
      "39  10.0  150.786 -247.713  16.981\n",
      "40  10.0  147.352 -242.509  17.039\n",
      "41  10.0  137.483 -226.524  16.929\n",
      "42  10.0  144.650 -235.989  16.918\n",
      "43  10.0  138.876 -229.195  16.802\n",
      "44  10.0  148.071 -241.616  16.743\n",
      "45  10.0  145.140 -236.894  16.686\n",
      "46  10.0  144.666 -234.177  16.737\n",
      "47  10.0  143.134 -231.063  16.765\n",
      "48  10.0  149.698 -239.678  16.762\n",
      "49  10.0  146.154 -232.732  16.609\n",
      "50  10.0  154.365 -244.616  16.356\n",
      "51  10.0  139.858 -223.071  16.347\n",
      "52  10.0  145.827 -230.837  16.272\n",
      "53  10.0  158.036 -246.830  16.308\n",
      "54  10.0  159.483 -248.836  16.259\n",
      "55  10.0  162.257 -251.131  16.249\n",
      "56  10.0  159.722 -248.703  16.184\n",
      "57  10.0  162.219 -249.978  16.159\n",
      "58  10.0  169.453 -259.772  16.123\n",
      "59  10.0  157.924 -243.240  16.087\n",
      "60  10.0  167.797 -256.692  16.071\n",
      "61  10.0  179.910 -273.908  15.763\n",
      "62  10.0  172.450 -262.795  15.857\n",
      "63  10.0  172.377 -262.356  15.710\n",
      "64  10.0  181.685 -276.043  15.736\n",
      "65  10.0  173.670 -265.071  15.681\n",
      "66  10.0  171.862 -263.329  15.689\n",
      "67  10.0  177.880 -271.603  15.592\n",
      "68  10.0  167.234 -256.704  15.472\n",
      "69  10.0  173.106 -265.006  15.430\n",
      "70  10.0  164.758 -253.147  15.522\n",
      "71  10.0  160.743 -248.681  15.398\n",
      "72  10.0  164.012 -254.953  15.396\n",
      "73  10.0  164.902 -254.221  15.394\n",
      "74  10.0  176.898 -274.161  15.311\n",
      "75  10.0  159.359 -248.398  15.548\n",
      "76  10.0  162.678 -256.941  15.524\n",
      "77  10.0  169.418 -266.777  14.977\n",
      "78  10.0  149.082 -238.676  15.195\n",
      "79  10.0  158.096 -254.621  15.403\n",
      "80  10.0  166.273 -268.139  15.077\n",
      "81  10.0  158.497 -258.744  15.020\n",
      "82  10.0  155.844 -256.918  14.909\n",
      "83  10.0  157.073 -259.576  14.862\n",
      "84  10.0  162.933 -269.362  14.972\n",
      "85  10.0  149.642 -249.718  14.791\n",
      "86  10.0  142.221 -239.506  14.652\n",
      "87  10.0  142.215 -240.994  14.682\n",
      "88  10.0  136.778 -233.790  14.714\n",
      "89  10.0  139.957 -241.395  14.686\n",
      "90  10.0  138.190 -240.815  14.847\n",
      "91  10.0  132.918 -232.321  14.707\n",
      "92  10.0  135.906 -238.334  14.973\n",
      "93  10.0  124.179 -226.958  14.731\n",
      "94  10.0  114.893 -209.645  14.539\n",
      "95  10.0  100.813 -186.028  15.410\n",
      "96  10.0  101.241 -188.916  15.143\n",
      "97  10.0  105.709 -197.596  14.230\n",
      "98  10.0  103.540 -194.158  14.305\n",
      "99  10.0   97.484 -184.029  14.391\n"
     ]
    }
   ],
   "source": [
    "print(result_df_va_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tr acc =  10.0\n",
      "Max va acc =  10.0\n"
     ]
    }
   ],
   "source": [
    "tr_acc = result_df_tr_all.values[:,0].squeeze()\n",
    "va_acc = result_df_va_all.values[:,0].squeeze()\n",
    "plt.figure(figsize=(20, 7))\n",
    "plt.plot(tr_acc, color='orange', label='train acc')\n",
    "plt.plot(va_acc, color='red', label='validataion acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"Max tr acc = \", np.max(tr_acc))\n",
    "print(\"Max va acc = \", np.max(va_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_mse = result_df_tr_all.values[:,3].squeeze()\n",
    "tr_spr_50 = 100*result_df_tr_all.values[:,4].squeeze()\n",
    "va_err = 5*result_df_va_all.values[:,-1].squeeze()\n",
    "plt.figure(figsize=(20, 7))\n",
    "plt.plot(tr_mse, color='orange', label='train mse')\n",
    "plt.plot(tr_spr_50, color='red', label='tr spr*100')\n",
    "plt.plot(va_err, color='blue', label='va_err*5')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
