{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doga/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/doga/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/doga/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/doga/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/doga/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/doga/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "matplotlib.style.use('ggplot')\n",
    "import sys, importlib as impL\n",
    "sys.path.insert(1,'/mnt/USB_HDD_1TB/GitHub/keyhandshapediscovery')\n",
    "import helperFuncs as funcH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bottleneck_acc(bottleneck_vec, lab_vec):\n",
    "    pred_vec = np.argmax(bottleneck_vec.T, axis=0).T.squeeze()\n",
    "    centroid_info_pdf = funcH.get_cluster_centroids(bottleneck_vec, pred_vec, kluster_centers=None, verbose=0)\n",
    "    _confMat_preds, kluster2Classes, kr_pdf, weightedPurity, cnmxh_perc = funcH.countPredictionsForConfusionMat(lab_vec, pred_vec, centroid_info_pdf=centroid_info_pdf, labelNames=None)\n",
    "    sampleCount = np.sum(np.sum(_confMat_preds))\n",
    "    acc = 100 * np.sum(np.diag(_confMat_preds)) / sampleCount\n",
    "    bmx, bmn = np.max(bottleneck_vec), np.min(bottleneck_vec)\n",
    "    return acc, bmx, bmn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the Argument Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add sparsity regularization: yes\n"
     ]
    }
   ],
   "source": [
    "#ap = argparse.ArgumentParser()\n",
    "#ap.add_argument('-e', '--epochs', type=int, default=10, help='number of epochs to train our network for')\n",
    "#ap.add_argument('-l', '--reg_param', type=float, default=0.001, help='regularization parameter `lambda`')\n",
    "#ap.add_argument('-sc', '--add_sparse', type=str, default='yes', help='whether to add sparsity contraint or not')\n",
    "#args = vars(ap.parse_args())\n",
    "epochs = 15  # args['epochs']\n",
    "reg_param = 0.001  # args['reg_param']\n",
    "add_sparsity = 'yes'  # args['add_sparse']\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "print(f\"Add sparsity regularization: {add_sparsity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here I will change the data loader per my need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the computation device\n",
    "def get_device():\n",
    "    return 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "FOLDERS = {\n",
    "    \"data\": '/mnt/USB_HDD_1TB/Datasets',\n",
    "    \"experiment\": '/mnt/USB_HDD_1TB/GitHub/keyhandshapediscovery/experiments/SPARSE_TORCH/sparse_torch_ae_03',\n",
    "}\n",
    "FOLDERS[\"model_save\"] = os.path.join(FOLDERS[\"experiment\"], \"model\")\n",
    "FOLDERS[\"decoder_image_path_tr\"] = os.path.join(FOLDERS[\"experiment\"], \"output_images_tr\")\n",
    "FOLDERS[\"decoder_image_path_va\"] = os.path.join(FOLDERS[\"experiment\"], \"output_images_va\")\n",
    "funcH.createDirIfNotExist(FOLDERS[\"model_save\"])\n",
    "funcH.createDirIfNotExist(FOLDERS[\"decoder_image_path_tr\"])\n",
    "funcH.createDirIfNotExist(FOLDERS[\"decoder_image_path_va\"])\n",
    "\n",
    "trainset = datasets.FashionMNIST(\n",
    "    root=FOLDERS[\"data\"],\n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "testset = datasets.FashionMNIST(\n",
    "    root=FOLDERS[\"data\"],\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    " \n",
    "# trainloader\n",
    "trainloader = DataLoader(\n",
    "    trainset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "#testloader\n",
    "testloader = DataLoader(\n",
    "    testset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the autoencoder model\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, loss_type):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    " \n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=784, out_features=256)\n",
    "        self.enc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.enc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.enc4 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.enc5 = nn.Linear(in_features=32, out_features=16)\n",
    " \n",
    "        # decoder \n",
    "        self.dec1 = nn.Linear(in_features=16, out_features=32)\n",
    "        self.dec2 = nn.Linear(in_features=32, out_features=64)\n",
    "        self.dec3 = nn.Linear(in_features=64, out_features=128)\n",
    "        self.dec4 = nn.Linear(in_features=128, out_features=256)\n",
    "        self.dec5 = nn.Linear(in_features=256, out_features=784)\n",
    "        \n",
    "        self.loss_type=loss_type\n",
    "        self.device = get_device()\n",
    " \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = F.relu(self.enc4(x))\n",
    "        bottleneck = F.relu(self.enc5(x))\n",
    "\n",
    "        # decoding\n",
    "        x = F.relu(self.dec1(bottleneck))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = F.relu(self.dec3(x))\n",
    "        x = F.relu(self.dec4(x))\n",
    "        x = F.relu(self.dec5(x))\n",
    "        return x, bottleneck\n",
    "model = SparseAutoencoder(loss_type='kl').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss function\n",
    "criterion = nn.MSELoss()\n",
    "# the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "Linear(in_features=128, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=128, bias=True)\n",
      "Linear(in_features=128, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=784, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the layers as a list\n",
    "model_children = list(model.children())\n",
    "[print(i) for i in model_children]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_l1(bottleneck):\n",
    "    return torch.mean(torch.abs(bottleneck))\n",
    "\n",
    "def loss_l2(bottleneck):\n",
    "    return torch.mean(torch.pow(bottleneck, torch.tensor(2.0).to(device))).sqrt()\n",
    "\n",
    "def kl_divergence(bottleneck):\n",
    "    rho = 0.05\n",
    "    bottleneck = torch.mean(torch.sigmoid(bottleneck), 1)  # sigmoid because we need the probability distributions\n",
    "    rho = torch.tensor([rho] * len(bottleneck)).to(device)\n",
    "    loss_ret_1 = torch.nn.functional.kl_div(bottleneck, rho, reduction='sum')\n",
    "    # torch.sum(rho * torch.log(rho / bottleneck) + (1 - rho) * torch.log((1 - rho) / (1 - bottleneck)))\n",
    "    return loss_ret_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sparse loss function\n",
    "def sparse_loss(autoencoder, images, print_info, loss_type):\n",
    "    loss = 0\n",
    "    values = images\n",
    "    for i in range(len(model_children)):\n",
    "        values = F.relu((model_children[i](values)))\n",
    "        #if print_info:\n",
    "            #print(i, ' shape=', values.shape)\n",
    "        if loss_type=='l1':\n",
    "            loss += loss_l1(values)\n",
    "        if loss_type=='l2':\n",
    "            loss += loss_l2(values)\n",
    "        if loss_type=='kl':\n",
    "            loss += kl_divergence(values)\n",
    "        if print_info:\n",
    "            print(loss_type,loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_decoded_image(img, name):\n",
    "    img = img.view(img.size(0), 1, 28, 28)\n",
    "    save_image(img, name)\n",
    "\n",
    "# define the training function\n",
    "def fit(model, dataloader, epoch, print_losses_fit):\n",
    "    print('Training')\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    lab_vec = []\n",
    "    bottleneck_vec = []\n",
    "    sparsity_loss_sum = 0\n",
    "       \n",
    "    for data in dataloader:\n",
    "        img, lb = data\n",
    "        lab_vec.append(lb)\n",
    "        \n",
    "        img = img.to(device)\n",
    "        img = img.view(img.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, bottleneck = model(img)\n",
    "        bottleneck_vec.append(bottleneck)\n",
    "        mse_loss = criterion(outputs, img)\n",
    "        #if print_losses_fit:\n",
    "            #print(\"mse_loss:\", mse_loss.to('cpu'))\n",
    "            #print(\"bottleneck:\", bottleneck.to('cpu'))\n",
    "        if add_sparsity == 'yes':\n",
    "            l1_loss = sparse_loss(model, img, print_losses_fit, model.loss_type)\n",
    "            sparsity_loss_sum += l1_loss.item()\n",
    "            # add the sparsity penalty\n",
    "            if print_losses_fit:\n",
    "                print(\"l1_loss:\", l1_loss.to('cpu'))\n",
    "            loss = mse_loss - reg_param * l1_loss\n",
    "        else:\n",
    "            loss = mse_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        print_losses_fit = False\n",
    "    \n",
    "    lab_vec = np.asarray(torch.cat(lab_vec).to(torch.device('cpu')))\n",
    "    bottleneck_vec = np.asarray(torch.cat(bottleneck_vec).to(torch.device('cpu')).detach().numpy())\n",
    "    acc, bmx, bmn = calc_bottleneck_acc(bottleneck_vec, lab_vec)\n",
    "    #print(\"tr bottleneck accuracy=\", acc, \", max=\", bmx, \", min=\", bmn, \", sparsity_loss_sum=\", sparsity_loss_sum)\n",
    "    print_dict = {\n",
    "        \"tr bottleneck accuracy\": acc,\n",
    "        \"tr bottleneck max value\": bmx,\n",
    "        \"tr bottleneck min value\": bmn,\n",
    "        \"tr sparsity_loss_sum\": sparsity_loss_sum,\n",
    "        \"tr running_loss\": running_loss,\n",
    "    }\n",
    "    print(print_dict)\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        difn = os.path.join(FOLDERS[\"decoder_image_path_tr\"], \"train\"+str(epoch).zfill(3)+\".png\")\n",
    "        save_decoded_image(outputs.cpu().data, difn)\n",
    "    return running_loss, sparsity_loss_sum, acc, bmx, bmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the validation function\n",
    "def validate(model, dataloader, epoch, print_losses_fit):\n",
    "    print('Validating')\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    lab_vec = []\n",
    "    bottleneck_vec = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            img, lb = data\n",
    "            lab_vec.append(lb)\n",
    "            img = img.to(device)\n",
    "            img = img.view(img.size(0), -1)\n",
    "            outputs, bottleneck = model(img)\n",
    "            bottleneck_vec.append(bottleneck)\n",
    "            loss = criterion(outputs, img)\n",
    "            running_loss += loss.item()\n",
    "    # save the reconstructed images every 5 epochs\n",
    "    lab_vec = np.asarray(torch.cat(lab_vec).to(torch.device('cpu')))\n",
    "    bottleneck_vec = np.asarray(torch.cat(bottleneck_vec).to(torch.device('cpu')).detach().numpy())\n",
    "    acc, bmx, bmn = calc_bottleneck_acc(bottleneck_vec, lab_vec)\n",
    "    print_dict = {\n",
    "        \"va bottleneck accuracy\": acc,\n",
    "        \"va bottleneck max value\": bmx,\n",
    "        \"va bottleneck min value\": bmn,\n",
    "        \"va running_loss\": running_loss,\n",
    "    }\n",
    "    print(print_dict)\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        outputs = outputs.view(outputs.size(0), 1, 28, 28).cpu().data\n",
    "        difn = os.path.join(FOLDERS[\"decoder_image_path_va\"], \"reconstruction\"+str(epoch).zfill(3)+\".png\")\n",
    "        save_image(outputs, difn)\n",
    "    return running_loss, acc, bmx, bmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this example has reduction sum and - at kl divergence\n",
      "Epoch 1 of 15\n",
      "Training\n",
      "kl tensor(-5.6333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-11.2439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-16.8471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-22.4500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-28.0579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-33.6733, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-39.2867, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-44.8937, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-50.4977, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-56.0979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "l1_loss: tensor(-56.0979, grad_fn=<CopyBackwards>)\n",
      "{'tr bottleneck accuracy': 15.066666666666666, 'tr bottleneck max value': 5.7653604, 'tr bottleneck min value': 0.0, 'tr sparsity_loss_sum': -107017.8413696289, 'tr running_loss': 271.3647473230958}\n",
      "Validating\n",
      "{'va bottleneck accuracy': 11.25, 'va bottleneck max value': 4.518143, 'va bottleneck min value': 0.0, 'va running_loss': 19.20363689213991}\n",
      "Epoch 2 of 15\n",
      "Training\n",
      "{'tr bottleneck accuracy': 30.421666666666667, 'tr bottleneck max value': 4.3959517, 'tr bottleneck min value': 0.0, 'tr sparsity_loss_sum': -106814.58051300049, 'tr running_loss': 214.28500317037106}\n",
      "Validating\n",
      "{'va bottleneck accuracy': 34.62, 'va bottleneck max value': 2.4526284, 'va bottleneck min value': 0.0, 'va running_loss': 16.92190232127905}\n",
      "Epoch 3 of 15\n",
      "Training\n",
      "{'tr bottleneck accuracy': 35.096666666666664, 'tr bottleneck max value': 2.4357758, 'tr bottleneck min value': 0.0, 'tr sparsity_loss_sum': -106577.35606765747, 'tr running_loss': 202.06086262315512}\n",
      "Validating\n",
      "{'va bottleneck accuracy': 32.21, 'va bottleneck max value': 2.4423103, 'va bottleneck min value': 0.0, 'va running_loss': 14.423132698982954}\n",
      "Epoch 4 of 15\n",
      "Training\n",
      "{'tr bottleneck accuracy': 32.12, 'tr bottleneck max value': 2.5931683, 'tr bottleneck min value': 0.0, 'tr sparsity_loss_sum': -106657.0952796936, 'tr running_loss': 189.19137862324715}\n",
      "Validating\n",
      "{'va bottleneck accuracy': 30.11, 'va bottleneck max value': 2.3633192, 'va bottleneck min value': 0.0, 'va running_loss': 13.389579974114895}\n",
      "Epoch 5 of 15\n",
      "Training\n",
      "{'tr bottleneck accuracy': 27.416666666666668, 'tr bottleneck max value': 2.4041176, 'tr bottleneck min value': 0.0, 'tr sparsity_loss_sum': -106578.4231376648, 'tr running_loss': 185.20069210231304}\n",
      "Validating\n",
      "{'va bottleneck accuracy': 26.72, 'va bottleneck max value': 2.2475739, 'va bottleneck min value': 0.0, 'va running_loss': 12.913761861622334}\n",
      "Epoch 6 of 15\n",
      "Training\n",
      "{'tr bottleneck accuracy': 23.261666666666667, 'tr bottleneck max value': 2.2053661, 'tr bottleneck min value': 0.0, 'tr sparsity_loss_sum': -106494.75009536743, 'tr running_loss': 182.60932417958975}\n",
      "Validating\n",
      "{'va bottleneck accuracy': 17.91, 'va bottleneck max value': 2.075694, 'va bottleneck min value': 0.0, 'va running_loss': 12.573233243077993}\n",
      "Epoch 7 of 15\n",
      "Training\n",
      "{'tr bottleneck accuracy': 23.59, 'tr bottleneck max value': 2.1672914, 'tr bottleneck min value': 0.0, 'tr sparsity_loss_sum': -106449.7332687378, 'tr running_loss': 180.74528309702873}\n",
      "Validating\n",
      "{'va bottleneck accuracy': 20.1, 'va bottleneck max value': 2.0721314, 'va bottleneck min value': 0.0, 'va running_loss': 12.262131193652749}\n",
      "Epoch 8 of 15\n",
      "Training\n",
      "{'tr bottleneck accuracy': 8.946666666666667, 'tr bottleneck max value': 2.140232, 'tr bottleneck min value': 0.0, 'tr sparsity_loss_sum': -106429.33570098877, 'tr running_loss': 178.3981493189931}\n",
      "Validating\n",
      "{'va bottleneck accuracy': 19.91, 'va bottleneck max value': 2.117399, 'va bottleneck min value': 0.0, 'va running_loss': 11.759842652827501}\n",
      "Epoch 9 of 15\n",
      "Training\n",
      "{'tr bottleneck accuracy': 18.481666666666666, 'tr bottleneck max value': 2.101457, 'tr bottleneck min value': 0.0, 'tr sparsity_loss_sum': -106404.87335968018, 'tr running_loss': 175.24189621210098}\n",
      "Validating\n",
      "{'va bottleneck accuracy': 21.4, 'va bottleneck max value': 1.9074193, 'va bottleneck min value': 0.0, 'va running_loss': 11.327097930014133}\n",
      "Epoch 10 of 15\n",
      "Training\n",
      "{'tr bottleneck accuracy': 22.845, 'tr bottleneck max value': 1.8633261, 'tr bottleneck min value': 0.0, 'tr sparsity_loss_sum': -106364.77462768555, 'tr running_loss': 172.56340446323156}\n",
      "Validating\n",
      "{'va bottleneck accuracy': 18.98, 'va bottleneck max value': 1.8013638, 'va bottleneck min value': 0.0, 'va running_loss': 10.93853559717536}\n",
      "Epoch 11 of 15\n",
      "Training\n",
      "{'tr bottleneck accuracy': 19.498333333333335, 'tr bottleneck max value': 1.8160522, 'tr bottleneck min value': 0.0, 'tr sparsity_loss_sum': -106323.02575683594, 'tr running_loss': 170.22644843906164}\n",
      "Validating\n",
      "{'va bottleneck accuracy': 23.36, 'va bottleneck max value': 1.6921136, 'va bottleneck min value': 0.0, 'va running_loss': 10.570728631690145}\n",
      "Epoch 12 of 15\n",
      "Training\n",
      "{'tr bottleneck accuracy': 19.748333333333335, 'tr bottleneck max value': 1.8436196, 'tr bottleneck min value': 0.0, 'tr sparsity_loss_sum': -106283.56670761108, 'tr running_loss': 168.47159604728222}\n",
      "Validating\n",
      "{'va bottleneck accuracy': 23.81, 'va bottleneck max value': 1.6065005, 'va bottleneck min value': 0.0, 'va running_loss': 10.289092164486647}\n",
      "Epoch 13 of 15\n",
      "Training\n",
      "{'tr bottleneck accuracy': 23.67, 'tr bottleneck max value': 1.7939374, 'tr bottleneck min value': 0.0, 'tr sparsity_loss_sum': -106253.36611175537, 'tr running_loss': 166.06762117892504}\n",
      "Validating\n",
      "{'va bottleneck accuracy': 21.96, 'va bottleneck max value': 1.583889, 'va bottleneck min value': 0.0, 'va running_loss': 9.836700320243835}\n",
      "Epoch 14 of 15\n",
      "Training\n",
      "{'tr bottleneck accuracy': 22.223333333333333, 'tr bottleneck max value': 1.6964872, 'tr bottleneck min value': 0.0, 'tr sparsity_loss_sum': -106224.4524230957, 'tr running_loss': 164.54741151630878}\n",
      "Validating\n",
      "{'va bottleneck accuracy': 24.5, 'va bottleneck max value': 1.5248489, 'va bottleneck min value': 0.0, 'va running_loss': 9.720030684024096}\n",
      "Epoch 15 of 15\n",
      "Training\n",
      "{'tr bottleneck accuracy': 24.743333333333332, 'tr bottleneck max value': 1.674583, 'tr bottleneck min value': 0.0, 'tr sparsity_loss_sum': -106208.58954238892, 'tr running_loss': 162.61036351323128}\n",
      "Validating\n",
      "{'va bottleneck accuracy': 24.4, 'va bottleneck max value': 1.5451106, 'va bottleneck min value': 0.0, 'va running_loss': 9.287075743079185}\n",
      "4.91 minutes\n"
     ]
    }
   ],
   "source": [
    "# train and validate the autoencoder neural network\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "start = time.time()\n",
    "print_losses_fit = True\n",
    "trn_bot_acc = []\n",
    "trn_spars_loss = []\n",
    "val_bot_acc = []\n",
    "lab_vec = []\n",
    "print(\"this example has reduction sum and - at kl divergence\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss, sparsity_loss_sum, acctr, bmxtr, _ = fit(model, trainloader, epoch, print_losses_fit)\n",
    "    val_epoch_loss, accva, bmxva, _ = validate(model, testloader, epoch, print_losses_fit)\n",
    "    trn_bot_acc.append(acctr)\n",
    "    trn_spars_loss.append(sparsity_loss_sum)\n",
    "    val_bot_acc.append(accva)\n",
    "    print_losses_fit = False\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "end = time.time()\n",
    " \n",
    "print(f\"{(end-start)/60:.3} minutes\")\n",
    "# save the trained model\n",
    "\n",
    "mofn = os.path.join(FOLDERS[\"model_save\"], \"sparse_ae_\"+str(epoch).zfill(3)+\".pth\")\n",
    "torch.save(model.state_dict(), mofn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
