{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doga/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/doga/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/doga/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/doga/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/doga/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/doga/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "matplotlib.style.use('ggplot')\n",
    "import sys, importlib as impL\n",
    "sys.path.insert(1,'/mnt/USB_HDD_1TB/GitHub/keyhandshapediscovery')\n",
    "import helperFuncs as funcH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bottleneck_acc(bottleneck_vec, lab_vec):\n",
    "    pred_vec = np.argmax(bottleneck_vec.T, axis=0).T.squeeze()\n",
    "    centroid_info_pdf = funcH.get_cluster_centroids(bottleneck_vec, pred_vec, kluster_centers=None, verbose=0)\n",
    "    _confMat_preds, kluster2Classes, kr_pdf, weightedPurity, cnmxh_perc = funcH.countPredictionsForConfusionMat(lab_vec, pred_vec, centroid_info_pdf=centroid_info_pdf, labelNames=None)\n",
    "    sampleCount = np.sum(np.sum(_confMat_preds))\n",
    "    acc = 100 * np.sum(np.diag(_confMat_preds)) / sampleCount\n",
    "    bmx, bmn = np.max(bottleneck_vec), np.min(bottleneck_vec)\n",
    "    return acc, bmx, bmn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the Argument Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add sparsity regularization: yes\n"
     ]
    }
   ],
   "source": [
    "#ap = argparse.ArgumentParser()\n",
    "#ap.add_argument('-e', '--epochs', type=int, default=10, help='number of epochs to train our network for')\n",
    "#ap.add_argument('-l', '--reg_param', type=float, default=0.001, help='regularization parameter `lambda`')\n",
    "#ap.add_argument('-sc', '--add_sparse', type=str, default='yes', help='whether to add sparsity contraint or not')\n",
    "#args = vars(ap.parse_args())\n",
    "epochs = 15  # args['epochs']\n",
    "reg_param = 0.001  # args['reg_param']\n",
    "add_sparsity = 'yes'  # args['add_sparse']\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "print(f\"Add sparsity regularization: {add_sparsity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here I will change the data loader per my need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the computation device\n",
    "def get_device():\n",
    "    return 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "FOLDERS = {\n",
    "    \"data\": '/mnt/USB_HDD_1TB/Datasets',\n",
    "    \"experiment\": '/mnt/USB_HDD_1TB/GitHub/keyhandshapediscovery/experiments/SPARSE_TORCH/sparse_torch_ae_02',\n",
    "}\n",
    "FOLDERS[\"model_save\"] = os.path.join(FOLDERS[\"experiment\"], \"model\")\n",
    "FOLDERS[\"decoder_image_path\"] = os.path.join(FOLDERS[\"experiment\"], \"outputs\", \"images\")\n",
    "funcH.createDirIfNotExist(FOLDERS[\"model_save\"])\n",
    "funcH.createDirIfNotExist(FOLDERS[\"decoder_image_path\"])\n",
    "\n",
    "trainset = datasets.FashionMNIST(\n",
    "    root=FOLDERS[\"data\"],\n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "testset = datasets.FashionMNIST(\n",
    "    root=FOLDERS[\"data\"],\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    " \n",
    "# trainloader\n",
    "trainloader = DataLoader(\n",
    "    trainset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "#testloader\n",
    "testloader = DataLoader(\n",
    "    testset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the autoencoder model\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, loss_type):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    " \n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=784, out_features=256)\n",
    "        self.enc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.enc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.enc4 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.enc5 = nn.Linear(in_features=32, out_features=16)\n",
    " \n",
    "        # decoder \n",
    "        self.dec1 = nn.Linear(in_features=16, out_features=32)\n",
    "        self.dec2 = nn.Linear(in_features=32, out_features=64)\n",
    "        self.dec3 = nn.Linear(in_features=64, out_features=128)\n",
    "        self.dec4 = nn.Linear(in_features=128, out_features=256)\n",
    "        self.dec5 = nn.Linear(in_features=256, out_features=784)\n",
    "        \n",
    "        self.loss_type=loss_type\n",
    "        self.device = get_device()\n",
    " \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        if self.loss_type=='this was kl here':\n",
    "            actFun = nn.Softmax(dim=1)\n",
    "            x = actFun(self.enc1(x))\n",
    "            x = actFun(self.enc2(x))\n",
    "            x = actFun(self.enc3(x))\n",
    "            x = actFun(self.enc4(x))\n",
    "            bottleneck = actFun(self.enc5(x))\n",
    "\n",
    "            # decoding\n",
    "            x = actFun(self.dec1(bottleneck))\n",
    "            x = actFun(self.dec2(x))\n",
    "            x = actFun(self.dec3(x))\n",
    "            x = actFun(self.dec4(x))\n",
    "            x = actFun(self.dec5(x))\n",
    "        else:\n",
    "            x = F.relu(self.enc1(x))\n",
    "            x = F.relu(self.enc2(x))\n",
    "            x = F.relu(self.enc3(x))\n",
    "            x = F.relu(self.enc4(x))\n",
    "            bottleneck = F.relu(self.enc5(x))\n",
    "\n",
    "            # decoding\n",
    "            x = F.relu(self.dec1(bottleneck))\n",
    "            x = F.relu(self.dec2(x))\n",
    "            x = F.relu(self.dec3(x))\n",
    "            x = F.relu(self.dec4(x))\n",
    "            x = F.relu(self.dec5(x))\n",
    "        return x, bottleneck\n",
    "model = SparseAutoencoder(loss_type='kl').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss function\n",
    "criterion = nn.MSELoss()\n",
    "# the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "Linear(in_features=128, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=128, bias=True)\n",
      "Linear(in_features=128, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=784, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the layers as a list\n",
    "model_children = list(model.children())\n",
    "[print(i) for i in model_children]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_l1(bottleneck):\n",
    "    return torch.mean(torch.abs(bottleneck))\n",
    "\n",
    "def loss_l2(bottleneck):\n",
    "    return torch.mean(torch.pow(bottleneck, torch.tensor(2.0).to(device))).sqrt()\n",
    "\n",
    "def kl_divergence(bottleneck):\n",
    "    rho = 0.05\n",
    "    bottleneck = torch.mean(torch.sigmoid(bottleneck), 1)  # sigmoid because we need the probability distributions\n",
    "    rho = torch.tensor([rho] * len(bottleneck)).to(device)\n",
    "    loss_ret_1 = torch.nn.functional.kl_div(bottleneck, rho)\n",
    "    # torch.sum(rho * torch.log(rho / bottleneck) + (1 - rho) * torch.log((1 - rho) / (1 - bottleneck)))\n",
    "    return loss_ret_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sparse loss function\n",
    "def sparse_loss(autoencoder, images, print_info, loss_type):\n",
    "    loss = 0\n",
    "    values = images\n",
    "    for i in range(len(model_children)):\n",
    "        values = F.relu((model_children[i](values)))\n",
    "        #if print_info:\n",
    "            #print(i, ' shape=', values.shape)\n",
    "        if loss_type=='l1':\n",
    "            loss += loss_l1(values)\n",
    "        if loss_type=='l2':\n",
    "            loss += loss_l2(values)\n",
    "        if loss_type=='kl':\n",
    "            loss += kl_divergence(values)\n",
    "        if print_info:\n",
    "            print(loss_type,loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_decoded_image(img, name):\n",
    "    img = img.view(img.size(0), 1, 28, 28)\n",
    "    save_image(img, name)\n",
    "\n",
    "# define the training function\n",
    "def fit(model, dataloader, epoch, print_losses_fit):\n",
    "    print('Training')\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    \n",
    "    lab_vec = []\n",
    "    bottleneck_vec = []\n",
    "    sparsity_loss_sum = 0\n",
    "       \n",
    "    for data in dataloader:\n",
    "        counter += 1\n",
    "        img, lb = data\n",
    "        lab_vec.append(lb)\n",
    "        \n",
    "        img = img.to(device)\n",
    "        img = img.view(img.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, bottleneck = model(img)\n",
    "        bottleneck_vec.append(bottleneck)\n",
    "        mse_loss = criterion(outputs, img)\n",
    "        if print_losses_fit:\n",
    "            print(\"mse_loss:\", mse_loss.to('cpu'))\n",
    "            #print(\"bottleneck:\", bottleneck.to('cpu'))\n",
    "        if add_sparsity == 'yes':\n",
    "            l1_loss = sparse_loss(model, img, print_losses_fit, model.loss_type)\n",
    "            sparsity_loss_sum += l1_loss.item()\n",
    "            # add the sparsity penalty\n",
    "            if print_losses_fit:\n",
    "                print(\"l1_loss:\", l1_loss.to('cpu'))\n",
    "            loss = mse_loss - reg_param * l1_loss\n",
    "        else:\n",
    "            loss = mse_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        print_losses_fit = False\n",
    "    \n",
    "    lab_vec = np.asarray(torch.cat(lab_vec).to(torch.device('cpu')))\n",
    "    bottleneck_vec = np.asarray(torch.cat(bottleneck_vec).to(torch.device('cpu')).detach().numpy())\n",
    "    acc, bmx, bmn = calc_bottleneck_acc(bottleneck_vec, lab_vec)\n",
    "    print(\"tr bottleneck accuracy=\", acc, \", max=\", bmx, \", min=\", bmn, \", sparsity_loss_sum=\", sparsity_loss_sum)\n",
    "    \n",
    "    epoch_loss = running_loss / counter\n",
    "    print(f\"Train Loss: {loss:.3f}\")\n",
    "    # save the reconstructed images every 5 epochs\n",
    "    if epoch % 2 == 0:\n",
    "        difn = os.path.join(FOLDERS[\"decoder_image_path\"], \"train\"+str(epoch).zfill(3)+\".png\")\n",
    "        save_decoded_image(outputs.cpu().data, difn)\n",
    "    return epoch_loss, sparsity_loss_sum, acc, bmx, bmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the validation function\n",
    "def validate(model, dataloader, epoch, print_losses_fit):\n",
    "    print('Validating')\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    lab_vec = []\n",
    "    bottleneck_vec = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            counter += 1\n",
    "            img, lb = data\n",
    "            lab_vec.append(lb)\n",
    "            img = img.to(device)\n",
    "            img = img.view(img.size(0), -1)\n",
    "            outputs, bottleneck = model(img)\n",
    "            bottleneck_vec.append(bottleneck)\n",
    "            loss = criterion(outputs, img)\n",
    "            running_loss += loss.item()\n",
    "    epoch_loss = running_loss / counter\n",
    "    print(f\"Val Loss: {loss:.3f}\")  \n",
    "    # save the reconstructed images every 5 epochs\n",
    "    lab_vec = np.asarray(torch.cat(lab_vec).to(torch.device('cpu')))\n",
    "    bottleneck_vec = np.asarray(torch.cat(bottleneck_vec).to(torch.device('cpu')).detach().numpy())\n",
    "    acc, bmx, bmn = calc_bottleneck_acc(bottleneck_vec, lab_vec)\n",
    "    print(\"va bottleneck accuracy=\", acc, \", max=\", bmx, \", min=\", bmn)\n",
    "    if epoch % 2 == 0:\n",
    "        outputs = outputs.view(outputs.size(0), 1, 28, 28).cpu().data\n",
    "        difn = os.path.join(FOLDERS[\"decoder_image_path\"], \"reconstruction\"+str(epoch).zfill(3)+\".png\")\n",
    "        save_image(outputs, difn)\n",
    "    return epoch_loss, acc, bmx, bmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 15\n",
      "Training\n",
      "mse_loss: tensor(0.1863, grad_fn=<CopyBackwards>)\n",
      "kl tensor(-0.1759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-0.3512, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-0.5263, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-0.7017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-0.8770, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-1.0524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-1.2277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-1.4030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-1.5781, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "kl tensor(-1.7531, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "l1_loss: tensor(-1.7531, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doga/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr bottleneck accuracy= 16.93166666666667 , max= 27.451878 , min= 0.0 , sparsity_loss_sum= -3409.5855226516724\n",
      "Train Loss: 0.060\n",
      "Validating\n",
      "Val Loss: 0.058\n",
      "va bottleneck accuracy= 15.57 , max= 26.979393 , min= 0.0\n",
      "Epoch 2 of 15\n",
      "Training\n",
      "tr bottleneck accuracy= 18.85 , max= 33.894054 , min= 0.0 , sparsity_loss_sum= -3451.3452265262604\n",
      "Train Loss: 0.048\n",
      "Validating\n",
      "Val Loss: 0.050\n",
      "va bottleneck accuracy= 23.93 , max= 32.777588 , min= 0.0\n",
      "Epoch 3 of 15\n",
      "Training\n",
      "tr bottleneck accuracy= 25.843333333333334 , max= 37.544743 , min= 0.0 , sparsity_loss_sum= -3462.8694034814835\n",
      "Train Loss: 0.048\n",
      "Validating\n",
      "Val Loss: 0.044\n",
      "va bottleneck accuracy= 26.14 , max= 36.621624 , min= 0.0\n",
      "Epoch 4 of 15\n",
      "Training\n",
      "tr bottleneck accuracy= 25.868333333333332 , max= 38.436123 , min= 0.0 , sparsity_loss_sum= -3466.4214891195297\n",
      "Train Loss: 0.044\n",
      "Validating\n",
      "Val Loss: 0.042\n",
      "va bottleneck accuracy= 25.57 , max= 38.5193 , min= 0.0\n",
      "Epoch 5 of 15\n",
      "Training\n",
      "tr bottleneck accuracy= 25.513333333333332 , max= 40.595238 , min= 0.0 , sparsity_loss_sum= -3469.4200431108475\n",
      "Train Loss: 0.038\n",
      "Validating\n",
      "Val Loss: 0.040\n",
      "va bottleneck accuracy= 25.01 , max= 40.08538 , min= 0.0\n",
      "Epoch 6 of 15\n",
      "Training\n",
      "tr bottleneck accuracy= 25.856666666666666 , max= 41.715702 , min= 0.0 , sparsity_loss_sum= -3470.451838493347\n",
      "Train Loss: 0.041\n",
      "Validating\n",
      "Val Loss: 0.039\n",
      "va bottleneck accuracy= 25.94 , max= 40.622093 , min= 0.0\n",
      "Epoch 7 of 15\n",
      "Training\n",
      "tr bottleneck accuracy= 26.725 , max= 42.2541 , min= 0.0 , sparsity_loss_sum= -3471.358934402466\n",
      "Train Loss: 0.036\n",
      "Validating\n",
      "Val Loss: 0.037\n",
      "va bottleneck accuracy= 25.93 , max= 40.53516 , min= 0.0\n",
      "Epoch 8 of 15\n",
      "Training\n",
      "tr bottleneck accuracy= 26.225 , max= 41.636425 , min= 0.0 , sparsity_loss_sum= -3472.064468741417\n",
      "Train Loss: 0.042\n",
      "Validating\n",
      "Val Loss: 0.036\n",
      "va bottleneck accuracy= 27.26 , max= 40.229336 , min= 0.0\n",
      "Epoch 9 of 15\n",
      "Training\n",
      "tr bottleneck accuracy= 27.89 , max= 41.408535 , min= 0.0 , sparsity_loss_sum= -3473.061320543289\n",
      "Train Loss: 0.038\n",
      "Validating\n",
      "Val Loss: 0.035\n",
      "va bottleneck accuracy= 27.58 , max= 39.229733 , min= 0.0\n",
      "Epoch 10 of 15\n",
      "Training\n",
      "tr bottleneck accuracy= 28.111666666666668 , max= 40.801834 , min= 0.0 , sparsity_loss_sum= -3473.6066588163376\n",
      "Train Loss: 0.033\n",
      "Validating\n",
      "Val Loss: 0.034\n",
      "va bottleneck accuracy= 26.8 , max= 38.31574 , min= 0.0\n",
      "Epoch 11 of 15\n",
      "Training\n",
      "tr bottleneck accuracy= 28.101666666666667 , max= 40.96165 , min= 0.0 , sparsity_loss_sum= -3474.1680402755737\n",
      "Train Loss: 0.031\n",
      "Validating\n",
      "Val Loss: 0.033\n",
      "va bottleneck accuracy= 27.9 , max= 38.45956 , min= 0.0\n",
      "Epoch 12 of 15\n",
      "Training\n",
      "tr bottleneck accuracy= 28.221666666666668 , max= 41.43914 , min= 0.0 , sparsity_loss_sum= -3475.4758751392365\n",
      "Train Loss: 0.028\n",
      "Validating\n",
      "Val Loss: 0.033\n",
      "va bottleneck accuracy= 26.99 , max= 38.67334 , min= 0.0\n",
      "Epoch 13 of 15\n",
      "Training\n",
      "tr bottleneck accuracy= 28.3 , max= 40.99561 , min= 0.0 , sparsity_loss_sum= -3476.540984272957\n",
      "Train Loss: 0.032\n",
      "Validating\n",
      "Val Loss: 0.032\n",
      "va bottleneck accuracy= 27.04 , max= 38.779213 , min= 0.0\n",
      "Epoch 14 of 15\n",
      "Training\n",
      "tr bottleneck accuracy= 28.328333333333333 , max= 41.225826 , min= 0.0 , sparsity_loss_sum= -3476.63260614872\n",
      "Train Loss: 0.031\n",
      "Validating\n",
      "Val Loss: 0.032\n",
      "va bottleneck accuracy= 27.13 , max= 39.54087 , min= 0.0\n",
      "Epoch 15 of 15\n",
      "Training\n",
      "tr bottleneck accuracy= 25.138333333333332 , max= 41.40322 , min= 0.0 , sparsity_loss_sum= -3476.7888107299805\n",
      "Train Loss: 0.028\n",
      "Validating\n",
      "Val Loss: 0.031\n",
      "va bottleneck accuracy= 28.13 , max= 39.240223 , min= 0.0\n",
      "4.97 minutes\n"
     ]
    }
   ],
   "source": [
    "# train and validate the autoencoder neural network\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "start = time.time()\n",
    "print_losses_fit = True\n",
    "trn_bot_acc = []\n",
    "trn_spars_loss = []\n",
    "val_bot_acc = []\n",
    "lab_vec = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss, sparsity_loss_sum, acctr, bmxtr, _ = fit(model, trainloader, epoch, print_losses_fit)\n",
    "    val_epoch_loss, accva, bmxva, _ = validate(model, testloader, epoch, print_losses_fit)\n",
    "    trn_bot_acc.append(acctr)\n",
    "    trn_spars_loss.append(sparsity_loss_sum)\n",
    "    val_bot_acc.append(accva)\n",
    "    print_losses_fit = False\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "end = time.time()\n",
    " \n",
    "print(f\"{(end-start)/60:.3} minutes\")\n",
    "# save the trained model\n",
    "\n",
    "mofn = os.path.join(FOLDERS[\"model_save\"], \"sparse_ae_\"+str(epoch).zfill(3)+\".pth\")\n",
    "torch.save(model.state_dict(), mofn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
