{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so I will have data as ndarray at bottleneck as N by D\n",
    "I will have the class labels acquired by kmeans\n",
    "I want to find correspondance samples as\n",
    "a) two random samples that are in cluster\n",
    "b) some sample and the center sample\n",
    "\n",
    "which wil be acquired as in and out indices by a function that takes nxd array and cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import importlib as impL\n",
    "import numpy as np\n",
    "from pandas import DataFrame as pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_dir(x):\n",
    "    return os.path.dirname(x)\n",
    "get_parent_dir(sys.path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(1,os.path.join(get_parent_dir(sys.path[0]),'vae_torch'))\n",
    "sys.path.insert(1,get_parent_dir(sys.path[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vae_torch_model as vtm\n",
    "import helperFuncs as funcH\n",
    "desktop_dir = funcH.getVariableByComputerName('desktop_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_from_saved_corr_file(fn = '/home/doga/Desktop/correspondance_find.npz'):\n",
    "    a = np.load(fn)\n",
    "    predictions = a['predictions']\n",
    "    bottleneck_vec = a['bottleneck_vec']\n",
    "    labels = a['labels']\n",
    "    print(\"loaded from file - \", fn)\n",
    "    print(predictions.shape)\n",
    "    print(bottleneck_vec.shape)\n",
    "    print(labels.shape)\n",
    "    return bottleneck_vec, predictions, labels\n",
    "\n",
    "def analyze_corresondance_results(correspondance_tuple, centroid_df, pred_vec, lab_vec):\n",
    "    if centroid_df is not None:\n",
    "        df = pd_df({'labels': lab_vec[np.asarray(centroid_df['sampleID'], dtype=int)],\n",
    "                    'klusterID': np.asarray(centroid_df['klusterID'], dtype=int),\n",
    "                    'sampleCounts': np.asarray(centroid_df['num_of_samples'], dtype=int)})\n",
    "        print('correspondance results({:}):'.format(len(correspondance_tuple[0])))\n",
    "        print(df.groupby(['labels'])[['labels', 'sampleCounts']].sum())\n",
    "    \n",
    "    corr_in_clust = pred_vec[correspondance_tuple[0]]\n",
    "    corr_ou_clust = pred_vec[correspondance_tuple[1]]\n",
    "    _confMat_corr_preds = confusion_matrix(corr_in_clust, corr_ou_clust)\n",
    "    acc_corr_preds = 100 * np.sum(np.diag(_confMat_corr_preds)) / np.sum(\n",
    "        np.sum(_confMat_corr_preds))\n",
    "    print(\"_confMat_corr_preds - acc({:6.4f})\".format(acc_corr_preds))\n",
    "\n",
    "    corr_in_labels = lab_vec[correspondance_tuple[0]]\n",
    "    corr_ou_labels = lab_vec[correspondance_tuple[1]]\n",
    "    _confMat_corr = confusion_matrix(corr_in_labels, corr_ou_labels)\n",
    "    acc_corr = 100 * np.sum(np.diag(_confMat_corr)) / np.sum(np.sum(_confMat_corr))\n",
    "    print(\"confMat - acc({:6.4f}), correspondance match:\\n\".format(acc_corr), pd_df(_confMat_corr))\n",
    "\n",
    "b_v = np.random.rand(10,3)\n",
    "#print(b_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez('/home/doga/Desktop/correspondance_find.npz', bottleneck_vec=bottleneck_vec, predictions=pred_vec, labels=lab_vec)\n",
    "bottleneck_vec, pred_vec, lab_vec = load_from_saved_corr_file(os.path.join(desktop_dir, 'correspondance_find_epoch1009_conf516.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impL.reload(funcH)\n",
    "def get_cluster_correspondance_ids_jupy(X, cluster_ids, correspondance_type=\"shuffle\", verbose=0):\n",
    "    # uses X to find the center sample\n",
    "    # returns inds_in, inds_out where:\n",
    "    # if correspondance_type=='shuffle'\n",
    "    # inds_in : shuffled indices of a cluster\n",
    "    # inds_out: shuffled indices of a cluster\n",
    "    # elseif correspondance_type=='centered'\n",
    "    # inds_in : some_sample_id\n",
    "    # inds_out: the center of cluster of that sample_id\n",
    "    centroid_df = funcH.get_cluster_centroids(ft=X, predClusters=cluster_ids, verbose=0)\n",
    "    uq_pr = np.unique(cluster_ids)\n",
    "    inds_in = []\n",
    "    inds_out = []\n",
    "    num_of_samples = []\n",
    "    for i in range(len(uq_pr)):\n",
    "        cluster_id = uq_pr[i]\n",
    "        cluster_inds = funcH.getInds(cluster_ids, i)\n",
    "        num_of_samples.append(len(cluster_inds))\n",
    "        if correspondance_type == 'shuffle':\n",
    "            iin_cur = cluster_inds.copy()\n",
    "            np.random.shuffle(iin_cur)\n",
    "            out_cur = cluster_inds.copy()\n",
    "            np.random.shuffle(out_cur)\n",
    "        elif 'knear' in correspondance_type:\n",
    "            if verbose > 0:\n",
    "                print(\"\\n***\\nknear-row{:}\\n\".format(i), cluster_inds)\n",
    "            k = int(correspondance_type.replace('knear', ''))\n",
    "            # look at the closest k samples for each sample\n",
    "            X_sub = X[cluster_inds, :]\n",
    "            k = np.minimum(len(cluster_inds), k)\n",
    "            d_inds, d_vals = funcH.get_dist_inds(X_sub, k=k, metric=\"euclidean\", verbose=0)\n",
    "            # d_inds are from 0 to len(cluster_inds)\n",
    "            # we want to switcth them with real cluster_inds\n",
    "            if verbose > 2:\n",
    "                print(\"cluster_inds:\\n\", cluster_inds)\n",
    "                print(\"d_inds in:\", d_inds)\n",
    "            # d_inds.shape = [len(cluster_inds), k]\n",
    "            # each row represents a sample and all columns represent its nearest neighbours\n",
    "            # so i need to have each corr and k neighbours as correspondant frames\n",
    "            sidx = np.array([cluster_inds, ] * k).T.flatten()\n",
    "            if verbose > 1:\n",
    "                print(\"i = \", i)\n",
    "                print(\"sidx = \\n\", sidx)\n",
    "            d_inds = cluster_inds[d_inds.flatten()]\n",
    "            if verbose > 1:\n",
    "                print(\"d_inds = \\n\", d_inds)\n",
    "            iin_cur = sidx.copy()\n",
    "            out_cur = d_inds.copy()\n",
    "        else:\n",
    "            center_sample_inds = centroid_df['sampleID'].iloc[i]\n",
    "            if verbose > 0:\n",
    "                print(\"cluster_id({:-3d}), sampleCount({:-4d}), centerSampleId({:-5d})\".format(int(cluster_id),\n",
    "                                                                                               len(cluster_inds),\n",
    "                                                                                               center_sample_inds))\n",
    "            # inds_in <--all sampleids except cluster center\n",
    "            # inds_out<--cluster sample id with length of inds_in\n",
    "            iin_cur = np.asarray(cluster_inds[np.where(center_sample_inds != cluster_inds)], dtype=int).squeeze()\n",
    "            out_cur = np.asarray(np.ones(iin_cur.shape) * center_sample_inds, dtype=int)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(\"iin_cur.shape{:}, out_cur.shape{:}\".format(iin_cur.shape, out_cur.shape))\n",
    "            #if i == 0:\n",
    "            print(\"iin=\", iin_cur)\n",
    "            print(\"out=\", out_cur)\n",
    "        inds_in.append(iin_cur)\n",
    "        inds_out.append(out_cur)\n",
    "\n",
    "    # first concatanate the lists into ndarray\n",
    "    inds_in = np.asarray(np.concatenate(inds_in), dtype=int)\n",
    "    inds_out = np.asarray(np.concatenate(inds_out), dtype=int)\n",
    "\n",
    "    if True:  # 'knear' not in correspondance_type:\n",
    "        # now a-b and b-a\n",
    "        ii_ret = np.asarray(np.concatenate([inds_in, inds_out]), dtype=int)\n",
    "        io_ret = np.asarray(np.concatenate([inds_out, inds_in]), dtype=int)\n",
    "    else:\n",
    "        ii_ret = inds_in.copy()\n",
    "        io_ret = inds_out.copy()\n",
    "\n",
    "    # now shuffle so that clusters not sorted in learning\n",
    "    print(\"shuffle all\")\n",
    "    p = np.random.permutation(len(ii_ret))\n",
    "    ii_ret = ii_ret[p]\n",
    "    io_ret = io_ret[p]\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\"inds_in.shape{:}, inds_out.shape{:}\".format(inds_in.shape, inds_out.shape))\n",
    "    centroid_df['num_of_samples'] = num_of_samples\n",
    "    return (ii_ret, io_ret), centroid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correspondance_type = 'knear1'\n",
    "impL.reload(funcH)\n",
    "correspondance_tuple, centroid_df = get_cluster_correspondance_ids_jupy(X=bottleneck_vec, cluster_ids=pred_vec,\n",
    "                                    correspondance_type=correspondance_type,\n",
    "                                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "analyze_corresondance_results(correspondance_tuple, centroid_df, pred_vec, lab_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impL.reload(funcH)\n",
    "D, sort_inds = funcH.create_dist_mat(x=bottleneck_vec, metric=\"euclidean\", verbose=0, use_less=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = np.float32(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impL.reload(funcH)\n",
    "dist_dict = funcH.get_linearized_distance_matrix(D,verbose=1, sort_dist=\"ascend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "analyze_corresondance_results((corr_A,corr_B), centroid_df=None, pred_vec=pred_vec, lab_vec=lab_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "impL.reload(funcH)\n",
    "corr_inds, centroid_df = funcH.get_cluster_correspondance_ids(b_v, cluster_ids=[0,0,0,0,1,1,2,2,1,1], correspondance_type=\"knear4\", verbose=0)\n",
    "print(pd_df({\"in\":corr_inds[0],\"out\":corr_inds[1]}))\n",
    "print(centroid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a,  centroid_df = get_cluster_correspondance_ids(bottleneck_vec, predictions, correspondance_type=\"shuffle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import vae_scripts as vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(corr_A.shape)\n",
    "print(corr_B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_B = np.squeeze(sort_inds[0,:])\n",
    "corr_A = np.arange(corr_B.size,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vs.run_compare_list(experiments_folder=\"/home/doga/DataFolder/vaesae_experiments_cor\",\n",
    "                     data_log_keys=['tr_te', 'te'],\n",
    "                     loss_key_list=[ 'bottleneck_kmeans', 'reconstruction'],\n",
    "                     exp_base_name='exp_linear_vae_FMNIST_is28_cf',\n",
    "                     ae_f_name_base='ae_ft_linear_vae_FMNIST_is28.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vae_utils as vu\n",
    "impL.reload(vu)\n",
    "vu.plot_cf(cf_int=540, data_log_keys = ['tr_te'], max_act_ep=20, plot_cnt = 3,\n",
    "            select_id_type='linspace', k_loss_disp_list={'bottleneck_kmeans'},\n",
    "            experiments_folder=None,  # '/home/doga/DataFolder/vaesae_experiments/FM'\n",
    "            exp_base_name=None,  # 'exp_conv_ae_simple_is28_cf'\n",
    "            plt_min_max_lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vae_utils as vu\n",
    "impL.reload(vu)\n",
    "vu.plot_cf(cf_int=541, data_log_keys = ['tr_te'], max_act_ep=None, plot_cnt = 3,\n",
    "            select_id_type='linspace', k_loss_disp_list={'bottleneck_kmeans'},\n",
    "            experiments_folder=None,  # '/home/doga/DataFolder/vaesae_experiments/FM'\n",
    "            exp_base_name=None,  # 'exp_conv_ae_simple_is28_cf'\n",
    "            plt_min_max_lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impL.reload(vu)\n",
    "vu.plot_cf(cf_int=532, data_log_keys = ['tr_te', 'te'], max_act_ep=None, plot_cnt = 3,\n",
    "            select_id_type='linspace', k_loss_disp_list={'bottleneck_kmeans'},\n",
    "            experiments_folder=None,  # '/home/doga/DataFolder/vaesae_experiments_cor'\n",
    "            exp_base_name=None,  # 'exp_linear_vae_FMNIST_is28_cf'\n",
    "            plt_min_max_lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_avg, silhouette_values_hl = funcH.calc_silhouette_params(bottleneck_vec, pred_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kluster_centroids = funcH.get_cluster_centroids(ft=bottleneck_vec, predClusters=pred_vec, kluster_centers=None, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_dict = funcH.analyze_silhouette_values(silhouette_values_hl, pred_vec, lab_vec,\n",
    "                                    centroid_info_pdf=kluster_centroids,\n",
    "                                    label_names=None, conf_plot_save_to='',\n",
    "                                    figsize=(12, 5), lw=[4, 3, 2], show_title=False, str_deg=15, str_size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. we have silhouette scores in silhouette_values_hl\n",
    "2. predictions and their ground truth labels at pred_vec and lab_vec\n",
    "3. what if for each cluster we only take the knear1 samples,\n",
    "   sort them by their sum of 2 silhouette values\n",
    "   and only approve if the couple is above average silhouette score?\n",
    "4. correspondance_tuple hass the original idx of values\n",
    "   silhouette_values_hl also is that way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correspondance_tuple)\n",
    "n = len(pred_vec)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuple_sihouette_score_sum = np.asarray([silhouette_values_hl[correspondance_tuple[0][i]]+silhouette_values_hl[correspondance_tuple[1][i]] for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuple_idx = np.argsort(-tuple_sihouette_score_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuple_sihouette_score_sum[tuple_idx[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tup_sor_a_idx = correspondance_tuple[0][tuple_idx]\n",
    "tup_sor_b_idx = correspondance_tuple[1][tuple_idx]\n",
    "lab_vec_a = lab_vec[tup_sor_a_idx]\n",
    "lab_vec_b = lab_vec[tup_sor_b_idx]\n",
    "print(lab_vec_a)\n",
    "print(lab_vec_b)\n",
    "_cn_a = []\n",
    "_cn_b = []\n",
    "uniq_class_cnt_perc_a = np.zeros(lab_vec_a.shape)\n",
    "uniq_class_cnt_perc_b = np.zeros(lab_vec_b.shape)\n",
    "for i in range(n):\n",
    "    if lab_vec_a[i] not in _cn_a:\n",
    "        _cn_a.append(lab_vec_a[i])\n",
    "    if lab_vec_b[i] not in _cn_b:\n",
    "        _cn_b.append(lab_vec_b[i])\n",
    "    uniq_class_cnt_perc_a[i] = len(_cn_a)\n",
    "    uniq_class_cnt_perc_b[i] = len(_cn_b)\n",
    "print(uniq_class_cnt_perc_a)\n",
    "print(uniq_class_cnt_perc_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(lab_vec_a==lab_vec_b)/len(lab_vec_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cumsum = np.cumsum(lab_vec_a == lab_vec_b) / np.cumsum(lab_vec_b==lab_vec_b)\n",
    "max_run_acc_idx = np.argmax(pred_cumsum)\n",
    "max_run_acc = pred_cumsum[max_run_acc_idx]\n",
    "print(\"max_run_acc({:6.4f}), at {:d}(%{:4.2f})\".format(max_run_acc, max_run_acc_idx, max_run_acc_idx/n))\n",
    "data_perc_vec = np.arange(0, len(pred_cumsum)) / len(pred_cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(1, figsize=(12,8), dpi=180)\n",
    "ax.plot(np.arange(max_run_acc_idx+5), pred_cumsum[:max_run_acc_idx+5], lw=2, label='accuracy', color='blue', ls='-', zorder=0)\n",
    "ax.plot(np.arange(max_run_acc_idx+5), tuple_sihouette_score_sum[tuple_idx[:max_run_acc_idx+5]], lw=2, label='silhouette_prec', color='green', ls='-', zorder=0)\n",
    "ax.plot(np.arange(max_run_acc_idx+5), uniq_class_cnt_perc_a[:max_run_acc_idx+5]/10.0, lw=1, label='silhouette_prec', color='purple', ls='-', zorder=0)\n",
    "ax.plot(np.arange(max_run_acc_idx+5), uniq_class_cnt_perc_b[:max_run_acc_idx+5]/10.0, lw=1, label='silhouette_prec', color='cyan', ls='-', zorder=0)\n",
    "plt.show()\n",
    "\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(1, figsize=(12,8), dpi=180)\n",
    "ax.plot(data_perc_vec, pred_cumsum, lw=2, label='accuracy', color='blue', ls='-', zorder=0)\n",
    "ax.plot(data_perc_vec, tuple_sihouette_score_sum[tuple_idx], lw=2, label='silhouette_prec', color='green', ls='-', zorder=0)\n",
    "ax.plot(data_perc_vec, uniq_class_cnt_perc_a/10.0, lw=3, label='silhouette_prec', color='purple', ls='-', zorder=0)\n",
    "ax.plot(data_perc_vec, uniq_class_cnt_perc_b/10.0, lw=1, label='silhouette_prec', color='cyan', ls='-', zorder=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mplcursors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "b_v = np.random.rand(4,1,10)\n",
    "print(b_v.shape[0])\n",
    "print(1 in b_v.shape)\n",
    "print(b_v.squeeze().shape)\n",
    "print(1 in b_v.squeeze().shape)\n",
    "weights = np.random.rand(4,)\n",
    "print(weights)\n",
    "print(weights.shape)\n",
    "torch.from_numpy(np.array([weights,]*10).T).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "514.85px",
    "left": "1433.42px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
